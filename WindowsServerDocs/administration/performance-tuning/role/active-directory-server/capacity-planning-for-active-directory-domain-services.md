---
title: Active Directory Domain Services の容量計画
description: AD DS の容量計画時に考慮する必要がある要因について詳しく説明します。
ms.prod: windows-server
ms.technology: performance-tuning-guide
ms.topic: article
ms.author: v-tea; kenbrunf
author: teresa-motiv
ms.date: 7/3/2019
ms.openlocfilehash: bc8486369d076573d249b9d5cfb0ba669619461e
ms.sourcegitcommit: d99bc78524f1ca287b3e8fc06dba3c915a6e7a24
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 07/27/2020
ms.locfileid: "87179228"
---
# <a name="capacity-planning-for-active-directory-domain-services"></a>Active Directory Domain Services の容量計画

このトピックは、最初は Ken Brumfield、Microsoft のシニアプレミアフィールドエンジニアによって作成されており、Active Directory Domain Services (AD DS) の容量計画に関する推奨事項を提供しています。

## <a name="goals-of-capacity-planning"></a>容量計画の目標

容量計画は、パフォーマンスインシデントのトラブルシューティングと同じではありません。 密接に関連していますが、まったく異なっています。 容量計画の目標は次のとおりです。

- 環境を適切に実装して運用する
- パフォーマンスの問題のトラブルシューティングに費やす時間を最小限に抑えます。

容量計画では、クライアントのパフォーマンス要件を満たすために、データセンター内のハードウェアのアップグレードに必要な時間に対応するために、ピーク期間中に40% のプロセッサ使用率のベースラインターゲットを持つ場合があります。 しかし、異常なパフォーマンスインシデントが通知されるように、監視アラートのしきい値は、5分間隔で90% に設定することができます。

違いは、容量管理のしきい値が継続的に超過している場合 (1 回限りのイベントが問題にならない場合)、容量の追加 (プロセッサの追加または高速化) がソリューションであるか、複数のサーバー間でのサービスの拡張がソリューションであるということです。 パフォーマンスアラートのしきい値は、クライアントエクスペリエンスが現在発生していることを示します。問題を解決するには、直ちに手順が必要です。

心配として、容量管理とは、自動車事故 (防御力、ブレーキが正しく機能していることの確認など) を防ぐことです。一方、パフォーマンスのトラブルシューティングは、事故が発生した後の警察、火災部門、および緊急医療の専門家によるものです。 これは、"防御運転" Active Directory スタイルです。

過去数年間、スケールアップシステムの容量計画のガイダンスは大幅に変更されています。 システムアーキテクチャにおける次の変更により、サービスの設計と拡張に関する基本的な想定に直面しました。

- 64ビットサーバープラットフォーム
- 仮想化
- 電力消費に対する注意の増加
- SSD ストレージ
- クラウド シナリオ

また、このアプローチは、サーバーベースの容量計画の演習から、サービスベースの容量計画の演習に移行しています。 Active Directory Domain Services (AD DS) は、多くの Microsoft およびサードパーティ製品がバックエンドとして使用する成熟した分散サービスであり、他のアプリケーションを実行するために必要な容量を確保するために適切に計画する最も重要な製品の1つになります。

### <a name="baseline-requirements-for-capacity-planning-guidance"></a>キャパシティプランニングガイダンスのベースライン要件

この記事では、次の基準要件が想定されています。

- 閲覧者は、 [Windows Server 2012 R2 のパフォーマンスチューニングガイドライン](/previous-versions//dn529133(v=vs.85))を読んで理解しています。
- Windows Server プラットフォームは、x64 ベースのアーキテクチャです。 ただし、Active Directory 環境が Windows Server 2003 x86 (現時点ではサポートライフサイクルの終わりを超えています) にインストールされており、サイズが 1.5 GB 未満で、メモリ内に簡単に保持できるディレクトリ情報ツリー (DIT) がある場合でも、この記事のガイドラインは引き続き適用されます。
- キャパシティプランニングは継続的なプロセスであり、環境が期待どおりに満たされているかどうかを定期的に確認する必要があります。
- ハードウェアのコストの変化に応じて、複数のハードウェアライフサイクルで最適化が行われます。 たとえば、メモリが安価になり、コアあたりのコストが削減されたり、異なるストレージオプションの料金が変化したりします。
- 1日のピーク時のビジー期間を計画します。 これは、30分または時間間隔で確認することをお勧めします。 それより大きいものは、"一時的なスパイク" によって実際のピークを隠すことがあり、それよりも少ない場合があります。
- 企業のハードウェアライフサイクルの過程で成長を計画します。 これには、段階的な方法でハードウェアをアップグレードまたは追加する方法、または 3 ~ 5 年ごとに完全な更新を行う方法が含まれる場合があります。 各には、Active Directory に対する負荷の増加量として "guess" が必要です。 収集された履歴データは、この評価に役立ちます。
- フォールトトレランスを計画します。 見積もり*n*を導き出した後、n 1、n *N* &ndash; *N* &ndash; 2、 *n* &ndash; *x*を含むシナリオを計画します。
  - 組織のニーズに応じて追加のサーバーを追加して、単一または複数のサーバーの損失がピーク時の最大容量を超えないようにします。
  - また、拡張計画とフォールトトレランス計画を統合する必要があることも考慮してください。 たとえば、負荷をサポートするために1つの DC が必要であるものの、その負荷が翌年に2倍になり、2つの Dc の合計が必要であると推定される場合は、フォールトトレランスをサポートするのに十分な容量がありません。 このソリューションは、3つの Dc から開始します。 また、予算が厳しい場合は 3 ~ 6 か月後に3番目の DC を追加することも計画できます。

    > [!NOTE]
    > Active Directory 対応アプリケーションを追加すると、アプリケーションサーバーまたはクライアントから負荷が発生したかどうかに関係なく、DC の負荷に大きな影響を与える可能性があります。

### <a name="three-step-process-for-the-capacity-planning-cycle"></a>容量計画サイクルの3段階のプロセス

容量計画では、まず、必要なサービスの品質を決定します。 たとえば、コアデータセンターでは高いレベルの同時実行性がサポートされており、ユーザーとアプリケーションの使用に関して一貫性のあるエクスペリエンスが必要になります。このためには、冗長性に注意し、システムとインフラストラクチャのボトルネックを最小限に抑える必要があります。 これに対し、少数のユーザーがいるサテライトの場所では、同じレベルの同時実行やフォールトトレランスは必要ありません。 そのため、サテライトオフィスは、基礎となるハードウェアおよびインフラストラクチャを最適化するために注意する必要がない場合があります。これにより、コストを削減できる可能性があります。 推奨事項とガイダンスはすべて、最適なパフォーマンスのためのものであり、要件が少ないシナリオでは選択的に緩和できます。

次の質問は仮想または物理ですか。 キャパシティプランニングの観点からは、適切な答えや間違った答えはありません。操作する変数のセットは異なります。 仮想化のシナリオは、次の2つのオプションのいずれかになります。

- ホストごとに1つのゲストを持つ "直接マッピング" (仮想化はサーバーからの物理ハードウェアの抽象化にのみ存在します)
- "共有ホスト"

テスト環境と運用環境のシナリオでは、"直接マッピング" シナリオを物理ホストと同じように扱うことができます。 ただし、「共有ホスト」では、後で詳しく説明するいくつかの考慮事項について説明します。 "共有ホスト" のシナリオでは、AD DS もリソースに対して競合することを意味し、そのためにはペナルティやチューニングに関する考慮事項があります。

これらの考慮事項を念頭に置いて、キャパシティプランニングサイクルは、反復的な3段階のプロセスです。

1. 既存の環境を測定し、システムのボトルネックが現在発生している場所を特定し、必要な容量を計画するために必要な環境基本を取得します。
1. 手順 1. で説明した条件に従って、必要なハードウェアを決定します。
1. 実装されているインフラストラクチャが仕様内で動作していることを監視および検証します。 この手順で収集されるデータの一部は、次のキャパシティプランニングのサイクルのベースラインになります。

### <a name="applying-the-process"></a>プロセスを適用しています

パフォーマンスを最適化するには、次の主要なコンポーネントが正しく選択されており、アプリケーションの負荷が調整されていることを確認します。

1. メモリ
1. ネットワーク
1. ストレージ
1. プロセッサ
1. Net Logon

AD DS の基本的な記憶域要件と、適切に記述されたクライアントソフトウェアの一般的な動作では、1万 ~ 2万ユーザーまでの環境で、物理的なハードウェアに関して容量計画に関する大量の投資を先送りすることができます。これは、ほとんどの最新のサーバークラスシステムが負荷を処理するためです。 ただし、次の表は、適切なハードウェアを選択するために既存の環境を評価する方法をまとめたものです。 各コンポーネントは、以降のセクションで詳細に分析され、AD DS 管理者がベースラインの推奨事項と環境固有のプリンシパルを使用してインフラストラクチャを評価するのに役立ちます。

一般的には次のとおりです。

- 現在のデータに基づくサイズ設定は、現在の環境に対してのみ正確です。
- どのような見積もりを行う場合でも、ハードウェアのライフサイクルを通じて拡張する必要があります。
- 現在のサイズを大きくして、大規模な環境に拡張するか、またはライフサイクルを通じて容量を追加するかを決定します。
- 仮想化については、すべての容量計画の原則と方法が適用されます。ただし、仮想化のオーバーヘッドは、どのドメイン関連にも追加する必要がある点が異なります。
- 予測しようとしているものと同様に、容量計画は正確な科学ではありません。 正確に計算し、100% の精度で計算することは期待しないでください。 ここでのガイダンスは、leanest の推奨事項です。安全性を強化するために容量を追加し、環境がターゲットにとどまることを継続的に検証します。

### <a name="data-collection-summary-tables"></a>データコレクションの概要テーブル

#### <a name="new-environment"></a>新しい環境

| コンポーネント | 見積もる |
|-|-|
|ストレージ/データベースのサイズ|ユーザーごとに 40 KB ~ 60 KB|
|RAM|データベース サイズ<br />基本オペレーティングシステムの推奨事項<br />サードパーティ アプリケーション|
|ネットワーク|1 GB|
|CPU|1000各コアの同時実行ユーザー|

#### <a name="high-level-evaluation-criteria"></a>高レベルの評価基準

| コンポーネント | 評価基準 | 計画に関する考慮事項 |
|-|-|-|
|ストレージ/データベースのサイズ|「[記憶域の制限](/previous-versions/windows/it-pro/windows-2000-server/cc961769(v=technet.10))」の「最適化によって解放されるディスク領域のログをアクティブ化するには」セクション| |
|ストレージ/データベースのパフォーマンス|<ul><li>"LogicalDisk ( *\<NTDS Database Drive\>* ) \ Avg disk sec/Read," "LogicalDisk ( *\<NTDS Database Drive\>* ) \ avg disk Sec/Write," "LogicalDisk ( *\<NTDS Database Drive\>* ) \ avg Disk sec/Transfer"</li><li>"LogicalDisk ( *\<NTDS Database Drive\>* ) \ Reads/sec," "LogicalDisk ( *\<NTDS Database Drive\>* ) \ Writes/sec," "LogicalDisk ( *\<NTDS Database Drive\>* ) \ transfer/sec"</li></ul>|<ul><li>ストレージには、次の2つの問題があります。<ul><li>使用可能な領域。現在のスピンドルベースと SSD ベースのストレージのサイズは、ほとんどの AD 環境には関係ありません。</li> <li>入出力 (IO) 操作を利用できます。多くの環境では、これは見過ごされることがよくあります。 ただし、NTDS データベース全体をメモリに読み込むのに十分な RAM がない環境のみを評価することが重要です。</li></ul><li>記憶域は複雑なトピックであるため、適切なサイズを設定するにはハードウェアベンダーの専門知識が必要です。 特に、SAN、NAS、iSCSI などのより複雑なシナリオがあります。 ただし、一般に、ストレージのギガバイトあたりのコストは、IO あたりのコストに直接影響します。<ul><li>Raid 5 では、Raid 1 よりもギガバイトあたりのコストが低くなりますが、Raid 1 では IO あたりのコストが低くなります。</li><li>スピンドルベースのハードドライブはギガバイトあたりのコストが低くなりますが、Ssd は IO あたりのコストが低くなります。</li></ul><li>コンピューターまたは Active Directory Domain Services サービスを再起動すると、拡張記憶域エンジン (ESE) のキャッシュが空になり、キャッシュの実行中にパフォーマンスがディスクにバインドされます。</li><li>ほとんどの環境では、AD はランダムなパターンでディスクに大量の i/o を読み取り、キャッシュと読み取りの最適化戦略の利点の多くを否定します。  さらに、AD のメモリには、ほとんどのストレージシステムキャッシュよりも大きなキャッシュが用意されています。</li></ul>
|RAM|<ul><li>データベース サイズ</li><li>基本オペレーティングシステムの推奨事項</li><li>サードパーティ アプリケーション</li></ul>|<ul><li>ストレージは、コンピューターで最も低速なコンポーネントです。 RAM に格納できる容量が多いほど、ディスクへのアクセスが必要になります。</li><li>オペレーティングシステム、エージェント (ウイルス対策、バックアップ、監視)、NTDS データベース、および長期間の拡張を格納するために十分な RAM が割り当てられていることを確認します。</li><li>RAM の容量を最大化することがコスト効率に優れていない環境 (サテライトの場所など) や、実現できない環境 (DIT が大きすぎる) の場合は、ストレージセクションを参照して、記憶域のサイズが適切に設定されていることを確認します。</li></ul>|
|ネットワーク|<ul><li>"ネットワークインターフェイス ( \* ) \ 受信バイト数/秒"</li><li>"ネットワークインターフェイス ( \* ) \ 送信バイト数/秒"|<ul><li>一般に、DC から遠くに送信されるトラフィックは、DC に送信されるトラフィックを超えます。</li><li>スイッチイーサネット接続は全二重であるため、受信および送信ネットワークトラフィックは個別にサイズ設定する必要があります。</li><li>Dc の数を統合すると、各 DC のクライアント要求に応答を送信するために使用される帯域幅が増加しますが、サイト全体に対して線形に近い距離になります。</li><li>サテライトの場所 Dc を削除する場合は、必ずサテライト DC の帯域幅をハブ Dc に追加し、それを使用して WAN トラフィックの量を評価してください。</li></ul>|
|CPU|<ul><li>"Logical Disk ( *\<NTDS Database Drive\>* ) \ Avg disk sec/Read"</li><li>"プロセス (lsass) \\ % Processor Time"</li></ul>|<ul><li>ボトルネックとして記憶域を排除した後、必要なコンピューティング能力の容量に対処します。</li><li>完全に線形ではありませんが、特定のスコープ (サイトなど) 内のすべてのサーバーで消費されるプロセッサコアの数を使用して、クライアントの負荷の合計をサポートするために必要なプロセッサの数を測定できます。 スコープ内のすべてのシステムで現在のサービスレベルを維持するために必要な最小要件を追加します。</li><li>電源管理に関連する変更、現在の環境から派生した影響番号など、プロセッサ速度の変更。 一般に、2.5 GHz プロセッサから 3 GHz プロセッサへの移行によって、必要な Cpu の数が減少することを正確に評価することはできません。</li></ul>|
|NetLogon|<ul><li>"Netlogon ( \* ) \ セマフォ獲得"</li><li>"Netlogon ( \* ) \ セマフォタイムアウト"</li><li>"Netlogon ( \* ) \ セマフォの平均保持時間"</li></ul>|<ul><li>Net Logon のセキュリティで保護されたチャネル/MaxConcurrentAPI は、NTLM 認証または PAC 検証を持つ環境にのみ影響します。 PAC 検証は、Windows Server 2008 より前のオペレーティングシステムバージョンでは既定で有効になっています。 これはクライアント設定なので、すべてのクライアントシステムでこのオプションがオフになるまで、Dc は影響を受けます。</li><li>フォレスト内の信頼関係を含む、かなり信頼度の高い認証を持つ環境では、サイズが適切でない場合にリスクが高くなります。</li><li>サーバーを統合すると、クロス信頼認証の同時実行性が向上します。</li><li>ユーザーが新しいクラスターノードに対してユーザーを再認証するときに、クラスターのフェールオーバーなど、急激に増加する必要があります。</li><li>個々のクライアントシステム (クラスターなど) でもチューニングが必要になる場合があります。</li></ul>|

## <a name="planning"></a>計画

長い間、AD DS のサイズ設定に関するコミュニティの推奨事項は、"データベースのサイズに応じて RAM を増設する" ということです。 ほとんどの場合、その推奨事項は、ほとんどの環境で考慮する必要があることです。 しかし、AD DS を使用するエコシステムは、1999で導入されて以来、AD DS の環境が非常に大きくなっています。 コンピューティング能力の向上と、x86 アーキテクチャから x64 アーキテクチャへの切り替えにより、物理ハードウェア上で AD DS を実行しているよりも多くの顧客にとって、パフォーマンスの曲に大きな影響がありました。仮想化の増加により、以前よりも多くのユーザーにチューニングの問題が発生しました。

次のガイダンスは、物理、仮想/物理ミックス、または純粋に仮想化されたシナリオのいずれで展開されるかに関係なく、Active Directory サービスとしての要求を決定し、計画する方法に関するものです。 そのため、ストレージ、メモリ、ネットワーク、プロセッサの4つの主要なコンポーネントについて、評価を分割します。 つまり、AD DS のパフォーマンスを最大にするために、可能な限りプロセッサの近くに到達することを目標としています。

## <a name="ram"></a>RAM

RAM にキャッシュできる量が多いほど、ディスクへのアクセスが必要になります。 サーバーのスケーラビリティを最大限に高めるには、RAM の最小容量を、現在のデータベースサイズの合計、SYSVOL の合計サイズ、オペレーティングシステムの推奨される量、およびエージェントに対するベンダーの推奨事項 (ウイルス対策、監視、バックアップなど) にする必要があります。 サーバーの有効期間中の増加に対応するために、追加の量を追加する必要があります。 これは、環境の変化に基づいて、データベースの増加量の見積もりに基づいて、環境的に主観的になります。

RAM の容量を最大化することがコスト効率に優れていない環境 (サテライトの場所など) や、実現できない環境 (DIT が大きすぎる) の場合は、ストレージのセクションを参照して、記憶域が適切に設計されていることを確認します。

メモリのサイズ変更に関する一般的なコンテキストでは、ページファイルのサイズが変更されています。 他のすべてのメモリに関連するものと同じコンテキストで、非常に低速なディスクへの移動を最小限に抑えることが目標となります。 そのため、"ページファイルのサイズはどのように設定する必要がありますか" という質問があります。 「ページングを最小化するために必要な RAM の量」 後者の質問に対する回答については、このセクションの残りの部分で説明します。 このため、一般的なオペレーティングシステムの推奨事項の領域にページファイルのサイズを設定するための説明のほとんどが残されています。また、メモリダンプ用にシステムを構成する必要があります。これは AD DS のパフォーマンスとは無関係です。

### <a name="evaluating"></a>評価中

ドメインコントローラー (DC) が必要とする RAM の容量は、次のような理由で実際に複雑な作業になります。

- 既存のシステムを使用して、LSASS がメモリ不足の状態でトリムする際に必要とされる RAM の量を測定する際に、エラーが発生する可能性が高くなります。この場合、deflating は不要です。
- 個々の DC がクライアントに "興味深い" ものをキャッシュする必要があるのは主観的な事実です。 つまり、Exchange server のみを含むサイトの DC にキャッシュする必要があるデータは、ユーザーの認証のみを行う DC にキャッシュする必要があるデータと大きく異なります。
- 各 DC の RAM をケースバイケースで評価するための労力は膨大であり、環境の変化に合わせて変化します。
- 推奨事項の背後にある条件は、情報に基づいた決定を行うのに役立ちます。
- RAM にキャッシュできる容量が多いほど、ディスクへのアクセスに必要な量が少なくなります。
- ストレージは、はるかに低速なコンピューターのコンポーネントです。 スピンドルベースおよび SSD ストレージメディア上のデータへのアクセスは、RAM 内のデータへのアクセスよりも1、000 x 10,000 倍の速度で動作します。

このため、サーバーのスケーラビリティを最大限に高めるために、RAM の最小容量は、現在のデータベースのサイズ、SYSVOL の合計サイズ、オペレーティングシステムの推奨される量、およびエージェントのベンダー推奨事項 (ウイルス対策、監視、バックアップなど) の合計になります。 サーバーの有効期間中の増加に対応するために、追加の量を追加します。 これは、データベースの増加の見積もりに基づいて、環境の主観的な主観的なものになります。 ただし、エンドユーザーの数が少ないサテライトの場所については、これらのサイトでほとんどの要求を処理できる限りキャッシュする必要がないため、これらの要件を緩和できます。

RAM の容量を最大化することがコスト効率に優れていない環境 (サテライトの場所など) や、実現できない環境 (DIT が大きすぎる) の場合は、ストレージセクションを参照して、記憶域のサイズが適切に設定されていることを確認します。

> [!NOTE]
> メモリのサイズを変更している間に、ページファイルのサイズが変更されます。 この目標は、非常に低速なディスクへの移行を最小限に抑えることを目的としているため、"ページファイルのサイズを設定するにはどうすればよいか" という疑問があります。 「ページングを最小化するために必要な RAM の量」 後者の質問に対する回答については、このセクションの残りの部分で説明します。 このため、一般的なオペレーティングシステムの推奨事項の領域にページファイルのサイズを設定するための説明のほとんどが残されています。また、メモリダンプ用にシステムを構成する必要があります。これは AD DS のパフォーマンスとは無関係です。

### <a name="virtualization-considerations-for-ram"></a>RAM の仮想化に関する考慮事項

ホストでメモリを過剰コミットすることは避けてください。 RAM の容量を最適化するための基本的な目標は、ディスクへの移動にかかる時間を最小限に抑えることです。 仮想化のシナリオでは、ゲストに割り当てられた RAM が多く、物理マシン上に存在する場合、コミットオーバーオーバーのメモリの概念が存在します。 これ自体では、このことは問題ではありません。 すべてのゲストによってアクティブに使用されているメモリの合計がホストの RAM 容量を超え、基になるホストがページングを開始したときに問題になります。 ドメインコントローラーがデータを取得するために ntds.dit に移動する場合、またはドメインコントローラーがデータを取得するためにページファイルに移動する場合、またはホストがディスクにアクセスして、ゲストが RAM にあると考えられるデータを取得する場合は、パフォーマンスがディスクバインドになります。

### <a name="calculation-summary-example"></a>計算の概要の例

|コンポーネント|推定メモリ (例)|
|-|-|
|基本オペレーティングシステムの推奨 RAM (Windows Server 2008)|2 GB|
|LSASS 内部タスク|200 MB|
|監視エージェント|100 MB|
|ウイルス対策|100 MB|
|データベース (グローバルカタログ)|8.5 GB |
|バックアップを実行するためのクッション、影響を及ぼさずにログオンする管理者|1 GB|
|合計|12 GB|

**推奨:16 GB**

時間の経過と共に、より多くのデータがデータベースに追加され、サーバーが 3 ~ 5 年の実稼働環境に配置される可能性があります。 33% の増加の推定に基づいて、16 GB は物理サーバーに配置するのに十分な RAM 容量です。 仮想マシンでは、どの設定を簡単に変更でき、RAM を VM に追加できます。その場合、12 GB から開始し、将来の監視とアップグレードの計画を立てることが妥当です。

## <a name="network"></a>ネットワーク

### <a name="evaluating"></a>評価中

このセクションでは、レプリケーショントラフィックに関する要求の評価について説明します。これは、WAN を通過するトラフィックに重点を置いており、 [Active Directory レプリケーショントラフィック](/previous-versions/windows/it-pro/windows-2000-server/bb742457(v=technet.10))で完全にカバーされています。これは、必要な合計帯域幅とネットワーク容量を評価することであり、クライアントクエリ、グループポリシーアプリケーションなどを含めます。 既存の環境の場合は、パフォーマンスカウンター "Network Interface ( \* ) \ Bytes Received/sec" と "Network interface ( \* )/bytes Sent/sec" を使用して収集できます。 15、30、または60分のネットワークインターフェイスカウンターのサンプル間隔。 一般に、適切な測定を行うには、あまり揮発性がありません。さらに多くの場合、毎日のピークが過剰になります。

> [!NOTE]
> 一般に、dc でのネットワークトラフィックの大半は、DC がクライアントクエリに応答するときに送信されます。 これは送信トラフィックに焦点を当てる理由ですが、受信トラフィックについては各環境を評価することをお勧めします。 同じ方法を使用して、受信ネットワークトラフィックの要件に対処して確認することもできます。 詳細については、サポート技術情報の記事[929851: Windows Vista および Windows Server 2008 で tcp/ip の既定の動的ポート範囲が変更されてい](https://support.microsoft.com/kb/929851)ます。

### <a name="bandwidth-needs"></a>帯域幅のニーズ

ネットワークのスケーラビリティの計画には、トラフィックの量とネットワークトラフィックからの CPU 負荷の2つの異なるカテゴリがあります。 これらの各シナリオは、この記事の他のトピックと比較して、簡単に説明されています。

サポートする必要があるトラフィックの量を評価する際に、ネットワークトラフィックの観点から AD DS の容量計画には、2つの固有のカテゴリがあります。 1つ目は、ドメインコントローラー間を通過するレプリケーショントラフィックであり、参照[Active Directory レプリケーショントラフィック](/previous-versions/windows/it-pro/windows-2000-server/bb742457(v=technet.10))について詳しく説明されており、AD DS の現在のバージョンにも関連しています。 2つ目は、サイト内のクライアントとサーバー間のトラフィックです。 を計画するための単純なシナリオの1つとして、サイト内のトラフィックは、クライアントから送信された大量のデータを基準として、クライアントから小さな要求を受け取ることが主にあります。 100 MB は通常、サイト内のサーバーあたり最大5000ユーザーの環境で十分です。 5000を超えるユーザーには、1 GB のネットワークアダプターを使用し、Receive Side Scaling (RSS) のサポートを受けることをお勧めします。 このシナリオを検証するには、特にサーバー統合のシナリオでは、 \* サイト内のすべての dc でネットワークインターフェイス () \ Bytes/sec を参照し、それらをまとめて追加し、適切な容量が確保されるようにドメインコントローラーのターゲット数で除算します。 これを行う最も簡単な方法は、Windows 信頼性とパフォーマンスモニター (旧称 Perfmon) の "積み上げ Area" ビューを使用して、すべてのカウンターのサイズを同じにすることです。

次の例を考えてみます (これは、一般的な規則が特定の環境に適用可能であることを検証するための本当に複雑な方法でもあります)。 次のような仮定が行われます。

- 目標は、できるだけ少ないサーバーにフットプリントを削減することです。 理想的には、1台のサーバーで負荷を処理し、冗長性を確保するために追加のサーバーをデプロイします (*N* + 1 シナリオ)。
- このシナリオでは、現在のネットワークアダプターは 100 MB のみをサポートし、スイッチされた環境に存在します。
  N シナリオ (DC の損失) では、ターゲットネットワーク帯域幅の最大使用率は60% です。
- 各サーバーには、約1万のクライアントが接続されています。

グラフのデータから得られたナレッジ (ネットワークインターフェイス ( \* )/送信バイト数/秒):

1. 営業日は約5:30 で始まり、7:00 PM に風になります。
1. ピーク時の最大値は 8:00 AM から 8:15 AM で、最もビジーな DC では25バイト/秒を超えて送信されます。
   > [!NOTE]
   > すべてのパフォーマンスデータは履歴です。 そのため、8:15 のピークデータポイントは8:00 から8:15 への負荷を示しています。
1. 最もビジーな DC には、4:00 AM より前のスパイクがあります。これは、異なるタイムゾーンからの負荷、またはバックアップなどのバックグラウンドのインフラストラクチャアクティビティを示す可能性があります。 8:00 AM のピークはこのアクティビティを超えているため、これは関係ありません。
1. サイトには5つのドメインコントローラーがあります。
1. 最大負荷は DC あたり約 5.5 MB/秒です。これは、100 MB 接続の44% を表します。 このデータを使用して、8:00 AM と 8:15 AM の間に必要な合計帯域幅が 28 MB/秒であることを推定できます。
   > [!NOTE]
   > ネットワークインターフェイスの送信/受信カウンターがバイト単位であり、ネットワーク帯域幅がビット単位で計測されるという事実に注意してください。 100 MB &divide; 8 = 12.5 mb、1 GB &divide; 8 = 128 mb。

わかり

1. この現在の環境は、60% のターゲット使用率で N + 1 レベルのフォールトトレランスを満たしています。 1つのシステムをオフラインにすると、サーバーごとの帯域幅が約 5.5 MB/秒 (44%) に移行されます。約 7 MB/秒 (56%)。
1. 以前に説明した1つのサーバーへの統合の目標に基づいて、この2つがターゲットの最大使用率を超え、理論的には 100 MB 接続の使用率を超えています。
1. 1 GB の接続では、これは合計容量の22% を表します。
1. *N* + 1 シナリオの通常の動作条件下では、クライアントの負荷は、サーバーあたり約 14 MB/秒、または合計容量の11% に均等に分散されます。
1. DC が使用できないときに容量が適切であることを保証するために、サーバーごとの通常の運用ターゲットは、サーバーあたり約30% のネットワーク使用率または 38 MB/秒になります。 フェールオーバーターゲットは、サーバーあたり60% のネットワーク使用率または 72 MB/秒になります。

要するに、システムの最終的な展開には、1 GB のネットワークアダプターが必要であり、負荷をサポートするネットワークインフラストラクチャに接続されている必要があります。 さらに、生成されるネットワークトラフィックの量が多いと、ネットワーク通信からの CPU 負荷が大きく影響し、AD DS の最大スケーラビリティが制限される可能性があることに注意してください。 この同じプロセスを使用して、DC への受信通信の量を見積もることができます。 しかし、受信トラフィックを基準とした送信トラフィックの predominance があれば、ほとんどの環境では教育機関の課題です。 サーバーあたり5000人を超えるユーザーがいる環境では、RSS のハードウェアサポートを確保することが重要です。 ネットワークトラフィックの量が多いシナリオでは、割り込み負荷の分散がボトルネックになることがあります。 これは、 \* \% cpu 間で均等に分散されているプロセッサ () 割り込み時間によって検出できます。 RSS 対応の Nic は、この制限を緩和し、スケーラビリティを向上させることができます。

> [!NOTE]
> 同様の方法を使用して、データセンターを統合する場合や、サテライトの場所にあるドメインコントローラーを廃止する場合に必要な追加容量を見積もることができます。 クライアントへの送信トラフィックと受信トラフィックを収集するだけで、WAN リンクに現在存在するトラフィック量になります。
>
> 場合によっては、トラフィックが低速であるため、予想よりも多くのトラフィックが発生する可能性があります。これは、証明書の確認が WAN でのアグレッシブなタイムアウトを満たしていない場合などです。 このため、WAN のサイズと使用率は反復的で継続的なプロセスである必要があります。

### <a name="virtualization-considerations-for-network-bandwidth"></a>ネットワーク帯域幅に関する仮想化に関する考慮事項

物理サーバーには、5000人を超えるユーザーをサポートするサーバーに対して 1 GB の推奨事項を作成するのが簡単です。 複数のゲストが基礎となる仮想スイッチインフラストラクチャの共有を開始したら、システム上のすべてのゲストをサポートするのに十分なネットワーク帯域幅がホストに確保されるように注意する必要があります。 これは、ホストコンピューターにネットワークインフラストラクチャを確保する拡張機能にすぎません。 これは、ネットワークが仮想スイッチを経由するネットワークトラフィックを持つホスト上のバーチャルマシンゲストとして実行されているドメインコントローラを包含しているか、物理スイッチに直接接続されているかに関係ありません。 仮想スイッチは、送信されるデータ量をアップリンクがサポートする必要があるコンポーネントの1つにすぎません。 そのため、スイッチにリンクされている物理ホストの物理ネットワークアダプターは、DC の負荷と、物理ネットワークアダプターに接続されている仮想スイッチを共有している他のすべてのゲストをサポートできる必要があります。

### <a name="calculation-summary-example"></a>計算の概要の例

|システム|ピーク時の帯域幅|
|-|-|
DC 1|6.5 MB/秒|
DC 2|6.25 MB/秒|
|DC 3|6.25 MB/秒|
|DC 4|5.75 MB/秒|
|DC 5|4.75 MB/秒|
|合計|28.5 MB/秒|

**推奨:72 mb/秒**(28.5 mb/秒を40%) で割る

|ターゲットシステム数|合計帯域幅 (上記から)|
|-|-|
|2|28.5 MB/秒|
|結果として得られる通常の動作|28.5 &divide; 2 = 14.25 MB/秒|

常に、時間の経過と共に、クライアントの負荷が増加し、この拡張を可能な限りできるだけ計画する必要があります。 計画対象として推奨される量は、50% のネットワークトラフィックの増加を見積もることができます。

## <a name="storage"></a>ストレージ

記憶域の計画では、2つのコンポーネントを構成します。

- 容量、またはストレージのサイズ
- パフォーマンス

非常に多くの時間とドキュメントが、キャパシティの計画に費やされ、パフォーマンスが完全に見落とされることがあります。 現在のハードウェアコストでは、ほとんどの環境は十分な大きさではないため、これらのうちのいずれかが実際に問題になります。また、大規模な環境では、サテライトの場所が過剰に使用される可能性があります。

### <a name="sizing"></a>サイズ変更

#### <a name="evaluating-for-storage"></a>ストレージの評価

13年前と比較して、Active Directory が導入された時点では、4 GB と 9 GB のドライブが最も一般的なドライブサイズである場合、Active Directory のサイズは、最大規模の環境全体では考慮されません。 180 GB の範囲で使用可能なハードドライブのサイズが最小であるため、オペレーティングシステム全体、SYSVOL、および ntds.dit を1つのドライブに簡単に収めることができます。 そのため、この領域での大量投資を廃止することをお勧めします。

推奨事項は、最適化を有効にするために、ntds.dit サイズの110% が使用可能であることを確認することだけです。 さらに、ハードウェアの寿命を越えた拡張のための設備を作成する必要があります。

最初の最も重要な考慮事項は、ntds.dit と SYSVOL の大きさを評価することです。 これらの測定値により、固定ディスクと RAM 割り当ての両方のサイズが変更されます。 これらのコンポーネントの (比較的) 低コストであるため、演算は厳密で正確である必要はありません。 既存の環境と新しい環境の両方についてこれを評価する方法については、[データストレージ](/previous-versions/windows/it-pro/windows-2000-server/cc961771(v=technet.10))シリーズの記事を参照してください。 具体的には、次の記事を参照してください。

- **既存の環境 &ndash; の場合**「[記憶域の制限](/previous-versions/windows/it-pro/windows-2000-server/cc961769(v=technet.10))」の「最適化によって解放されるディスク領域のログをアクティブ化するには」セクション
- **新しい環境 &ndash; の場合**「 [Active Directory ユーザーと組織単位の増加見積もり」と](/previous-versions/windows/it-pro/windows-2000-server/cc961779(v=technet.10))いうタイトルの記事。

  > [!NOTE]
  > この記事は、Windows 2000 で Active Directory がリリースされた時点で行われたデータサイズの見積もりに基づいています。 環境内のオブジェクトの実際のサイズを反映するオブジェクトサイズを使用します。

複数のドメインを持つ既存の環境を確認する場合、データベースのサイズに違いがある可能性があります。 この値が true の場合は、最小のグローバルカタログ (GC) と非 GC のサイズを使用します。

データベースのサイズは、オペレーティングシステムのバージョンによって異なります。 Windows Server 2003 などの以前のオペレーティングシステムを実行している Dc は、特にごみ箱や資格情報の移動 Active Directory などの機能が有効になっている場合に、Windows Server 2008 R2 などのより新しいオペレーティングシステムを実行する DC よりもデータベースサイズが小さくなります。

> [!NOTE]
  >
>- 新しい環境については、Active Directory ユーザーと組織単位の増加見積もりの見積もりで、10万ユーザー (同じドメイン内) が約 450 MB の領域を消費していることが示されていることに注意してください。 値が設定されている属性は、合計金額に大きな影響を与える可能性があることに注意してください。 属性には、サードパーティ製品と Microsoft 製品 (Microsoft Exchange Server、Lync など) の両方で多数のオブジェクトが設定されます。 環境内の製品のポートフォリオに基づく評価が推奨されますが、最大規模の環境全体について正確な見積もりを行うための計算とテストの詳細については、実際には時間と労力があまり重要ではない可能性があります。
>- オフライン最適化を有効にするために、ntds.dit サイズの110% が空き領域として使用可能であることを確認し、3 ~ 5 年のハードウェア寿命を超えて拡張を計画します。 ストレージのコストがどれくらいになるかについては、ストレージを300% で推定すると、ストレージの割り当てが安全であり、オフラインでの最適化が必要になる可能性があります。

#### <a name="virtualization-considerations-for-storage"></a>記憶域の仮想化に関する考慮事項

複数の仮想ハードディスク (VHD) ファイルが1つのボリュームに割り当てられているシナリオでは、適切な領域が予約されていることを確認するために、少なくとも 210% (DIT + 110% 空き領域の 100%) の固定サイズのディスクを使用します。

#### <a name="calculation-summary-example"></a>計算の概要の例

|評価段階から収集されたデータ| |
|-|-|
|Ntds.dit のサイズ|35 GB|
|オフラインでのデフラグを許可する修飾子|2.1|
|必要なストレージの合計|73.5 GB|

> [!NOTE]
> 必要な記憶域は、SYSVOL、オペレーティングシステム、ページファイル、一時ファイル、ローカルにキャッシュされたデータ (インストーラーファイルなど)、およびアプリケーションに必要な記憶域に加えて必要です。

### <a name="storage-performance"></a>Storage performance (ストレージのパフォーマンス)

#### <a name="evaluating-performance-of-storage"></a>記憶域のパフォーマンスの評価

どのコンピューターでも最も低速なコンポーネントとして、記憶域はクライアントエクスペリエンスに最大の悪影響を与える可能性があります。 RAM のサイズ設定に関する推奨事項を十分に活用できない環境では、パフォーマンスの見落とし計画ストレージの結果が壊滅的になる可能性があります。  また、記憶域テクノロジの複雑さと種類によって、別の物理ディスクにおける "オペレーティングシステム、ログ、およびデータベースの配置" という長期にわたるベストプラクティスが、便利なシナリオでは制限されるため、障害のリスクがさらに高まります。  これは、"ディスク" が専用のスピンドルであり、この許可された i/o を分離できるという前提に基づいているためです。  この真を実現するこのような仮定は、の導入には関係ありません。

- 新しいストレージの種類と仮想化および共有ストレージのシナリオ
- 記憶域ネットワーク (SAN) 上の共有スピンドル
- SAN またはネットワークに接続されたストレージ上の VHD ファイル
- ソリッドステートドライブ
- 階層型ストレージアーキテクチャ (SSD ストレージ層では、より大きなスピンドルベースのストレージをキャッシュするなど)

つまり、基になるストレージアーキテクチャと設計に関係なく、すべてのストレージパフォーマンスの取り組みの最終的な目標は、1秒あたりの入力/出力操作 (IOPS) の必要量を確保し、それらの IOPS が許容時間内で発生することを保証することです。 このセクションでは、記憶域ソリューションが適切に設計されていることを確認するために、基になる記憶域の AD DS 要求を評価する方法について説明します。  現在のストレージテクノロジの可変性を考慮して、記憶域ベンダーと連携して、十分な IOPS を確保することをお勧めします。  ローカルに接続されたストレージを使用するシナリオについては、「従来のローカルストレージのシナリオを設計する方法」の基本について付録 C を参照してください。  このプリンシパルは、一般により複雑なストレージ層に適用されます。また、バックエンドストレージソリューションをサポートするベンダーのダイアログにも役立ちます。

- 幅広いストレージオプションを使用できる場合は、ハードウェアサポートチームまたはベンダーの専門知識を活用して、特定のソリューションが AD DS のニーズを満たすことを確認することをお勧めします。 次の数値は、ストレージスペシャリストに提供される情報です。

データベースが大きすぎて RAM に保持できない環境については、パフォーマンスカウンターを使用して、サポートする必要がある i/o の量を決定します。

- LogicalDisk ( \* ) \ Avg Disk sec/Read (たとえば、ntds.dit が D: に格納されている場合)ドライブの完全なパスは、LogicalDisk (D: \ Avg Disk sec/Read) になります。
- LogicalDisk ( \* ) \ ディスクの平均書き込み秒数
- LogicalDisk ( \* ) \ Avg Disk sec/Transfer
- LogicalDisk ( \* ) \ 読み取り/秒
- LogicalDisk ( \* ) \ Writes/sec
- LogicalDisk ( \* ) \ 転送/秒

これらは、現在の環境の需要をベンチマークするために、15/30/60 分間隔でサンプリングする必要があります。

#### <a name="evaluating-the-results"></a>結果の評価

> [!NOTE]
> データベースからの読み取りに焦点を当てます。これは通常、要求が最も厳しいコンポーネントであるため、LogicalDisk ( *\<NTDS Log\>* ) \ Avg Disk sec/Write と LogicalDisk ( *\<NTDS Log\>* ) \ writes/sec) を置き換えることで、ログファイルへの書き込みに同じロジックを適用できます。
>
> - LogicalDisk ( *\<NTDS\>* ) \ Avg Disk sec/Read は、現在のストレージのサイズが適切に設定されているかどうかを示します。  結果がディスクの種類のディスクアクセス時間とほぼ同じ場合、LogicalDisk ( *\<NTDS\>* ) \ Reads/sec は有効なメジャーです。  バックエンドのストレージの製造元の仕様を確認します。ただし、LogicalDisk ( *\<NTDS\>* )/Avg Disk sec/Read の適切な範囲は、ほぼ次のようになります。
>   - 7200– 9 ~ 12.5 ミリ秒 (ミリ秒)
>   - 1万– 6 ~ 10 ミリ秒
>   - 15000– 4 ~ 6 ミリ秒
>   - SSD – 1 ~ 3 ミリ秒
>   - > [!NOTE]
>     > ストレージのパフォーマンスが15ミリ秒で20ミリ秒 (ソースによって異なります) に低下することを示す推奨事項があります。  上記の値と他のガイダンスの違いは、上記の値が通常の動作範囲であることです。  その他の推奨事項は、クライアントエクスペリエンスが大幅に低下し、顕著になるタイミングを特定するためのトラブルシューティングガイダンスです。  詳細については、付録 C を参照してください。
> - LogicalDisk ( *\<NTDS\>* ) \ Reads/sec は、実行されている i/o の量です。
>   - LogicalDisk ( *\<NTDS\>* ) \ Avg Disk sec/Read がバックエンドストレージの最適な範囲内にある場合は、LogicalDisk ( *\<NTDS\>* ) \ Reads/sec を直接使用してストレージのサイズを設定できます。
>   - LogicalDisk ( *\<NTDS\>* ) \ Avg Disk sec/Read がバックエンドストレージの最適な範囲内にない場合は、次の式に従って追加の i/o が必要になります。
>     > (LogicalDisk ( *\<NTDS\>* ) \ Avg Disk sec/Read) &divide;(物理メディアのディスクアクセス時間) &times;(LogicalDisk ( *\<NTDS\>* ) \ Avg Disk sec/Read)

考慮事項:

- サーバーの RAM の容量が最適に構成されている場合は、これらの値が計画の目的で不正確になることに注意してください。  これらは誤って上位にあるため、最悪のシナリオとして使用することもできます。
- RAM を追加または最適化すると、読み取り i/o (LogicalDisk () \ 読み取り/秒の量が減少し *\<NTDS\>* ます。 つまり、ストレージソリューションは、最初に計算されたとおりに堅牢である必要はありません。  残念ながら、この一般的なステートメントよりも具体的には、クライアントの負荷に依存しているため、一般的なガイダンスを提供できません。  最適なオプションは、RAM を最適化した後でストレージのサイズを調整することです。

#### <a name="virtualization-considerations-for-performance"></a>パフォーマンスに関する仮想化に関する考慮事項

前述のすべての仮想化の説明と同様に、ここで重要なのは、基になる共有インフラストラクチャが、基になる共有メディアとそれに対するすべての経路を使用して、DC の負荷とその他のリソースをサポートできることを確認することです。 これは、物理ドメインコントローラーが、他のサーバーまたはアプリケーションと同様に、SAN、NAS、または iSCSI インフラストラクチャ上で同じ基になるメディアを共有しているか、基になるメディアを共有している SAN、NAS、または iSCSI インフラストラクチャへのパススルーアクセスを使用しているか、またはゲストが共有メディアに存在する VHD ファイルを、NAS、または iSCSI インフラストラクチャ。 計画の演習では、基になるメディアがすべてのコンシューマーの総負荷をサポートできることを確認します。

また、ゲストの観点からは、走査する必要がある追加のコードパスがあるため、ホストを経由してどのストレージにもアクセスする必要がある場合にパフォーマンスに影響があります。 もちろん、記憶域パフォーマンステストでは、仮想化がホストシステムのプロセッサ使用率に対する主観的なスループットに影響を与えることを示しています (「付録 A: CPU サイズ条件」を参照)。これは、ゲストが要求するホストのリソースによって影響を受けることが明らかです。 これは、仮想化されたシナリオにおける処理のニーズに関する仮想化に関する考慮事項に寄与します (「[処理のための仮想化に関する考慮事項](#virtualization-considerations-for-processing)」を参照

これをさらに複雑にすると、さまざまなストレージオプションを使用でき、パフォーマンスに影響を与える可能性があります。 物理から仮想に移行する場合の安全な推定方法として、乗数1.10 を使用して、パススルー記憶域、SCSI アダプター、IDE など、Hyper-v で仮想化されたゲストのさまざまな記憶域オプションを調整します。 異なるストレージシナリオ間で転送するときに行う必要がある調整は、記憶域がローカル、SAN、NAS、iSCSI のどれであるかに関係ありません。

#### <a name="calculation-summary-example"></a>計算の概要の例

通常の動作状態で正常なシステムに必要な i/o の量を確認するには、次の操作を行います。

- *\<NTDS Database Drive\>* ピーク期間中15分間の LogicalDisk () \ 転送/秒
- 基になる記憶域の容量が超過している記憶域に必要な i/o の量を確認するには、次のようにします。
  >*必要な IOPS* = (LogicalDisk ( *\<NTDS Database Drive\>* ) \ Avg Disk sec/read &divide; *\<Target Avg Disk sec/Read\>* ) &times; LogicalDisk ( *\<NTDS Database Drive\>* ) \ read/sec

|カウンター|値|
|-|-|
|実際の LogicalDisk ( *\<NTDS Database Drive\>* ) \ Avg Disk sec/Transfer|. 02 秒 (20 ミリ秒)|
|Target LogicalDisk ( *\<NTDS Database Drive\>* ) \ Avg Disk sec/Transfer|01秒|
|使用可能な IO の変化の乗数|0.02 &divide; 0.01 = 2|

|値の名前|値|
|-|-|
|LogicalDisk ( *\<NTDS Database Drive\>* ) \ 転送/秒|400|
|使用可能な IO の変化の乗数|2|
|ピーク期間中に必要な合計 IOPS|800|

キャッシュが必要とされる速度を決定するには、次のようにします。

- キャッシュのウォームアップに許容される最大時間を決定します。 これは、ディスクからデータベース全体を読み込むために必要な時間か、またはデータベース全体を RAM に読み込むことができないシナリオでは、RAM をいっぱいにするための最大時間になります。
- 空白を除いて、データベースのサイズを決定します。  詳細については、「[ストレージの評価](#evaluating-for-storage)」を参照してください。
- データベースサイズを 8 KB で割ります。これは、データベースの読み込みに必要な合計 Io 数になります。
- 合計 Io を、定義された期間内の秒数で割ります。

ESE が固定キャッシュ AD DS サイズを使用するように構成されていない場合は、以前に読み込まれたページが削除され、既定では変数キャッシュサイズが使用されるため、計算された精度は正確ではないことに注意してください。

|収集するデータポイント|値
|-|-|
|ウォームの許容最大時間|10分 (600 秒)
|データベース サイズ|2 GB|

|計算ステップ|Formula|結果|
|-|-|-|
|ページ内のデータベースのサイズを計算する|(2 GB &times; 1024 &times; 1024) =*データベースのサイズ (KB* )|2097152 KB|
|データベース内のページ数を計算する|2097152 KB &divide; 8 kb =*ページ数*|262144ページ|
|キャッシュを完全にウォームするために必要な IOPS を計算する|262144ページ &divide; 600 秒 = *IOPS が必要*|437 IOPS|

## <a name="processing"></a>処理中

### <a name="evaluating-active-directory-processor-usage"></a>Active Directory のプロセッサ使用率の評価

ほとんどの環境では、記憶域、RAM、およびネットワークが、「計画」セクションの説明に従って適切に調整された後、処理容量の管理は最も注意が必要なコンポーネントになります。 必要な CPU 容量の評価には、次の2つの課題があります。

- 環境内のアプリケーションが共有サービスインフラストラクチャで適切に動作しているかどうかについては、「Microsoft Active Directory 対応のアプリケーションを効率的に作成する」または「下位の SAM 呼び出しから LDAP 呼び出しへの移行」に記載されている「高コストで非効率的な検索の追跡」セクションで説明されています。

  大規模な環境では、このことが重要な理由は、コード化されていないアプリケーションは CPU 負荷の変動を大きくする可能性があります。つまり、他のアプリケーションからの CPU 時間を過剰に消費し、容量のニーズを人為的に増やし、Dc に負荷を分散均等ます。
- AD DS は、潜在的なクライアントが多数存在する分散環境であるため、"単一のクライアント" のコストを見積もることは、使用パターンと、AD DS を利用するアプリケーションの種類または量に起因する、環境の主観的な主観的なものです。 つまり、ネットワークのセクションと同様に、広範な適用性のために、環境で必要とされる合計容量を評価する観点から、この方法をお勧めします。

既存の環境については、ストレージのサイズ設定について既に説明したように、ストレージのサイズが適切に設定されているため、プロセッサの負荷に関するデータが有効であることが前提となっています。 繰り返しますが、システムのボトルネックが記憶域のパフォーマンスではないことを確認することが重要です。 ボトルネックが存在し、プロセッサが待機している場合、ボトルネックが解消されると、アイドル状態になります。  プロセッサの待機状態が削除されると、定義上、データを待機する必要がなくなるため、CPU 使用率が増加します。 このため、パフォーマンスカウンター "Logical Disk ( *\<NTDS Database Drive\>* ) \ Avg disk sec/Read" および "Process (lsass) \\ % Processor Time" を収集します。 " \\ Logical disk () \ Avg disk sec/Read" が 10 ~ 15 ミリ秒を超える場合、"Process (lsass)% Processor Time" のデータは人為的に低下し *\<NTDS Database Drive\>* ます。これは、Microsoft サポートが記憶域関連のパフォーマンスの問題をトラブルシューティングするために使用する一般的なしきい値です。 前と同様に、サンプリング間隔は15、30、または60分のいずれかにすることをお勧めします。 一般に、適切な測定を行うには、あまり揮発性がありません。さらに多くの場合、毎日のピークが過剰になります。

### <a name="introduction"></a>はじめに

ドメインコントローラーのキャパシティプランニングを計画するために、処理能力には最も注意を払って理解する必要があります。 パフォーマンスを最大にするためにシステムのサイズを変更する場合、ボトルネックとなるコンポーネントが常に存在し、適切にサイズ設定されたドメインコントローラーがプロセッサになります。

環境の要求がサイトごとに確認される [ネットワーク] セクションと同様に、要求されたコンピューティング容量についても同じことを行う必要があります。 ネットワークのセクションとは異なり、利用可能なネットワークテクノロジが通常の需要をはるかに超える場合は、CPU 容量のサイズ設定に注意してください。  中程度のサイズの環境として。同時に数千のユーザーを超えると、CPU の負荷が大幅に増大する可能性があります。

残念ながら、AD を利用するクライアントアプリケーションには大きなばらつきがあるため、CPU あたりのユーザーの一般的な推定は、すべての環境に誤解ことができません。 具体的には、ユーザーの行動とアプリケーションプロファイルの対象となります。 そのため、各環境のサイズを個別に設定する必要があります。

#### <a name="target-site-behavior-profile"></a>ターゲットサイトの動作プロファイル

既に説明したように、サイト全体の容量を計画する場合、目標は、容量設計が*N* + 1 の設計をターゲットにすることです。これは、ピーク期間に1つのシステムで障害が発生した場合に、妥当なレベルでサービスを継続できるようにするためです。 つまり、"*N*" のシナリオでは、すべてのボックスの負荷が100% 未満である必要があります (さらに、より低い、80%)ピーク期間中。

また、サイト内のアプリケーションとクライアントがドメインコントローラーを特定するためのベストプラクティスを使用している (つまり、 [DsGetDcName 機能](https://docs.microsoft.com/windows/win32/api/dsgetdc/nf-dsgetdc-dsgetdcnamea)を使用している) 場合、クライアントは、いくつかの要因によって一時的なスパイクが比較的均等に分散されている必要があります。

次の例では、次の仮定が行われています。

- サイト内の5つの各 Dc には、4つの Cpu が搭載されています。
- 営業時間内の合計ターゲット CPU 使用率は、通常の動作状態 ("*n* + 1") と 60% ("*n*") の場合は40% です。 バックアップソフトウェアやその他のメンテナンスで使用可能なすべてのリソースを消費することが想定されているため、営業時間外にターゲット CPU 使用率が80% になります。

![CPU 使用率グラフ](media/capacity-planning-considerations-cpu-chart.png)

各 Dc のグラフ内のデータを分析し `(Processor Information(_Total)\% Processor Utility)` ます。

- ほとんどの場合、負荷は比較的均等に分散されており、クライアントが DC ロケーターを使用していて、検索が適切に記述されていると予想されます。
- 10% のスパイクは5分間にわたっており、サイズは20% です。 一般に、キャパシティプランターゲットを超過しない限り、これらを調査することは価値がありません。
- すべてのシステムのピーク時間は約 8:00 AM と 9:15 AM です。 約 5:00 AM から約 5:00 PM までのスムーズな移行により、これは通常、ビジネスサイクルを示しています。 5:00 PM と 4:00 AM の間のボックス化されたシナリオでの CPU 使用率のランダムなスパイクは、容量計画の問題の範囲外となります。

  > [!NOTE]
  > 適切に管理されたシステムでは、バックアップソフトウェアが実行されている、システム全体のウイルス対策スキャン、ハードウェアまたはソフトウェアのインベントリ、ソフトウェアまたは修正プログラムの展開などが急増していると考えられます。 ピークユーザーのビジネスサイクルの範囲外であるため、ターゲットは超過しません。

- 各システムが約40% であり、すべてのシステムの Cpu の数が同じであるか、1つの障害が発生したか、またはオフラインになった場合、残りのシステムは、推定53% で実行されます (システム D の40% の負荷は均等に分割され、システム A とシステム C の既存の40% 負荷に 多くの理由から、この線形の想定は完全に正確ではありませんが、ゲージに十分な精度を提供します。

  **代替シナリオ–** 40% で実行されている2つのドメインコントローラー: 1 つのドメインコントローラーで障害が発生した場合、残りの CPU の推定 CPU は80% になります。 これまでに、容量計画について前述したしきい値を超えています。また、上のロードプロファイルに表示されるヘッドルームの量が、10% から20% に極端に制限されています。つまり、スパイクによって "*N*" のシナリオで DC が90% から100% になり、応答性が確実に低下し

### <a name="calculating-cpu-demands"></a>CPU 要求の計算

"プロセス \\ % Processor Time" パフォーマンスオブジェクトカウンタは、アプリケーションのすべてのスレッドが CPU に費やした合計時間を合計し、経過したシステム時間の合計によって割ります。 これにより、マルチ CPU システム上のマルチスレッドアプリケーションは100% の CPU 時間を超える可能性があり、"Processor Information% processor Utility" とはまったく異なる解釈がされ \\ ます。 実際には、 \\ プロセスの要求をサポートするために必要な100% で実行されている cpu の数として "プロセス (lsass)% Processor Time" を表示できます。 値が200% の場合は、フル AD DS の負荷をサポートするために、それぞれ100% の2つの Cpu が必要であることを意味します。 100% の容量で実行される CPU は、Cpu および電力とエネルギーの消費量の観点から最もコスト効率の高いものですが、付録 A で詳しく説明されている多くの理由により、システムが100% で実行されていない場合にマルチスレッドシステムでの応答性が向上します。

クライアント負荷の一時的な急増に対応するには、システム容量の 40% ~ 60% のピーク時 CPU を対象にすることをお勧めします。 上記の例を使用すると、AD DS (lsass プロセス) の負荷に対して 3.33 (60% target) と 5 (40% target) の Cpu が必要になることを意味します。 ベースオペレーティングシステムおよびその他の必要なエージェント (ウイルス対策、バックアップ、監視など) の要求に従って、に追加の容量を追加する必要があります。 エージェントの影響は環境ごとに評価する必要がありますが、1つの CPU の5% から10% の間の推定を行うことができます。 現在の例では、ピーク期間中は 3.43 (60% target) と 5.1 (40% target) の Cpu が必要であることが示されています。

これを行う最も簡単な方法は、Windows 信頼性とパフォーマンスモニター (perfmon) の "積み上げ Area" ビューを使用して、すべてのカウンターのサイズを同じにすることです。

想定:

- 目標は、フットプリントをできるだけ少ないサーバーに減らすことです。 理想的には、1台のサーバーで負荷を分散し、冗長性を確保するために追加のサーバーを追加します (*N* + 1 シナリオ)。

![Lsass プロセスのプロセッサ時間グラフ (全プロセッサ経由)](media/capacity-planning-considerations-proc-time-chart.png)

グラフ内のデータから得られたナレッジ (プロセス (lsass) \\ % Processor Time):

- 営業日は約7:00 で始まり、5:00 PM に減少します。
- ピーク時のピーク時間は、午前9:30 時から11:00 時です。
  > [!NOTE]
  > すべてのパフォーマンスデータは履歴です。 9:15 のピークデータポイントは、9:00 から9:15 への負荷を示します。
- 7:00 AM より前のスパイクがあります。これは、異なるタイムゾーンからの負荷、またはバックアップなどのバックグラウンドのインフラストラクチャアクティビティを示す可能性があります。 9:30 AM のピークはこのアクティビティを超えているため、これは関係ありません。
- サイトには3つのドメインコントローラーがあります。

最大負荷では、lsass は、1つの CPU の約485%、または100% で実行されている4.85 の Cpu を消費します。 前述のように、これは、AD DS についてサイトに12.25 の Cpu が必要であることを意味します。 バックグラウンドプロセスについて、上記の提案を5% から10% に追加します。これにより、現在のサーバーを置き換えると、同じ負荷をサポートするために約 12.30 ~ 12.35 の Cpu が必要になります。 現在、成長に関する環境の推定を考慮する必要があります。

### <a name="when-to-tune-ldap-weights"></a>LDAP の重みを調整する場合

場合によっては、複数のシナリオでの[Ldapsrvweight](/previous-versions/windows/it-pro/windows-2000-server/cc957291(v=technet.10))の調整を考慮する必要があります。 容量計画のコンテキスト内では、アプリケーションまたはユーザーの負荷が均等に分散されていない場合や、基になるシステムが機能面で均等に分散されていない場合に、この処理が行われます。 キャパシティプランニング以外の理由は、この記事の範囲外です。

LDAP の重みを調整するには、次の2つの一般的な理由があります。

- PDC エミュレーターは、ユーザーまたはアプリケーションの負荷の動作が均等に分散されていないすべての環境に影響する例です。 グループポリシー管理ツールなどの PDC エミュレーターを対象とする特定のツールとアクション、認証エラーや信頼関係の確立などでの2回目の試行では、PDC エミュレーターの CPU リソースが、サイト内の他の場所よりも高い頻度で要求されることがあります。
  - PDC エミュレーターの負荷を軽減し、他のドメインコントローラーの負荷を増やすことで負荷を均等に分散できるようにするために、CPU 使用率に顕著な違いがある場合にのみ、これをチューニングすると便利です。
  - この場合、PDC エミュレーターの場合は、50と75の間の LDAPSrvWeight を設定します。
- サイトの Cpu 数 (および速度) が異なるサーバー。  たとえば、2 8 コアサーバーと 1 4 コアサーバーがあるとします。  最後のサーバーは、他の2台のサーバーのプロセッサの半分を搭載しています。  これは、適切に分散されたクライアント負荷によって、4コアボックスの平均 CPU 負荷が8コアボックスの約2倍に増加することを意味します。
  - たとえば、2 8 コアのボックスは40% で実行され、4コアのボックスは80% で実行されます。
  - また、このシナリオでは、1 8-core box が失われた場合の影響についても検討します。特に、4コアのボックスはオーバーロードされるという事実です。

#### <a name="example-1---pdc"></a>例 1-PDC

| |既定値を使用した使用率|新しい LdapSrvWeight|新しい使用率の推定|
|-|-|-|-|
|DC 1 (PDC エミュレーター)|53%|57|40%|
|DC 2|33%|100|40%|
|DC 3|33%|100|40%|

ここでキャッチされるのは、PDC エミュレーターの役割が、特にサイト内の別のドメインコントローラーに転送または移動された場合、新しい PDC エミュレーターが大幅に増加することです。

「[ターゲットサイトの動作プロファイル](#target-site-behavior-profile)」セクションの例を使用すると、サイト内の3つのドメインコントローラーすべてに4つの cpu があることが前提となります。 ドメインコントローラーの1つに8個の Cpu がある場合、通常の状況ではどうなりますか。 使用率が40%、使用率が20% のドメインコントローラーが2つあります。 これは問題ではありませんが、負荷のバランスを少し上げることができます。 これを実現するには、LDAP の重みを活用します。  シナリオ例を次に示します。

#### <a name="example-2---differing-cpu-counts"></a>例 2-異なる CPU 数

| |プロセッサ情報 \\  % &nbsp; プロセッサユーティリティ (_Total)<br />既定値を使用した使用率|新しい LdapSrvWeight|新しい使用率の推定|
|-|-|-|-|
|4-CPU DC 1|40|100|30%|
|4-CPU DC 2|40|100|30%|
|8-CPU DC 3|20|200|30%|

ただし、これらのシナリオには十分注意してください。 上の図に示すように、数学は非常に優れていて、紙で見栄えがよく見られます。 ただし、この記事全体を通して、"*N* + 1" のシナリオを計画することが最も重要です。 1つの DC がオフラインになった場合の影響は、すべてのシナリオで計算する必要があります。 負荷分散が均等に行われる直前のシナリオでは、"*N*" シナリオ中に60% の負荷が発生するように、すべてのサーバー間で負荷が均等に分散されているので、比率が一貫しているため、ディストリビューションは問題なく動作します。 PDC エミュレーターのチューニングシナリオを見て、一般にユーザーまたはアプリケーションの負荷が不均衡なシナリオでは、効果は大きく異なります。

| |チューニング使用率|新しい LdapSrvWeight|新しい使用率の推定|
|-|-|-|-|
|DC 1 (PDC エミュレーター)|40%|85|47%|
|DC 2|40%|100|53%|
|DC 3|40%|100|53%|

### <a name="virtualization-considerations-for-processing"></a>処理のための仮想化に関する考慮事項

容量計画には、仮想化環境で行う必要がある2つの層があります。 ホストレベルでは、以前のドメインコントローラー処理のために概要を示したビジネスサイクルの識別と同様に、ピーク期間中のしきい値を特定する必要があります。 基になるプリンシパルはホストコンピューターに対して同じであるため、物理マシン上の CPU で AD DS スレッドを取得するために CPU 上のゲストスレッドをスケジュールすると、基になるホストで同じ40% から60% の目標を設定することをお勧めします。 次の層のゲスト層では、スレッドのスケジューリングのプリンシパルは変更されていないため、ゲスト内の目標は40% から60% の範囲にとどまります。

直接マップされたシナリオでは、ホストごとに1つのゲストを使用します。この時点までに行われたすべての容量計画は、基になるホストオペレーティングシステムの要件 (RAM、ディスク、ネットワーク) に追加する必要があります。 共有ホストのシナリオでは、テストは、基になるプロセッサの効率に10% の影響があることを示します。 つまり、1つのサイトで40% のターゲットで10個の Cpu が必要な場合、すべての "*N*" ゲストで割り当てることができる仮想 cpu の容量は11になります。 物理サーバーと仮想サーバーの分散が混在しているサイトでは、修飾子は Vm にのみ適用されます。 たとえば、サイトに "*N* + 1" というシナリオがある場合、10個の cpu を搭載した物理サーバーまたは直接マップされたサーバーの1つに、ホスト上の11個の cpu があるゲストに相当します。これは、ドメインコントローラー用に11個の cpu が予約されています。

AD DS の負荷をサポートするために必要な CPU 数量の分析と計算全体を通して、物理ハードウェアとして購入できるものにマップされる Cpu の数は必ずしも完全にはマップされません。 仮想化により、切り上げる必要がなくなります。 仮想化により、CPU を簡単に VM に追加できるため、コンピューティング能力をサイトに追加するために必要な労力が削減されます。 追加の Cpu をゲストに追加する必要がある場合に、基になるハードウェアを利用できるように、必要なコンピューティング能力を正確に評価する必要がなくなります。  常に、需要の増加に備えて計画と監視を行うようにしてください。

### <a name="calculation-summary-example"></a>計算の概要の例

|システム|ピーク時の CPU|
|-|-|-|
|DC 1|120%|
|DC 2|147%|
|Dc 3|218%|
|使用されている CPU の合計|485%|

|ターゲットシステム数|合計帯域幅 (上記から)|
|-|-|
|40% ターゲットで必要な Cpu|4.85 &divide; . 4 = 12.25|

この点が重要であるため、*増加を計画する*ことを忘れないでください。 次の3年間で50% の増加が想定されている場合、この環境では、3年間に 18.375 Cpu (12.25 1.5) が必要になり &times; ます。 代替計画は、最初の年の後に確認し、必要に応じて追加容量を追加します。

### <a name="cross-trust-client-authentication-load-for-ntlm"></a>NTLM 用のクロス信頼クライアント認証の負荷

#### <a name="evaluating-cross-trust-client-authentication-load"></a>相互信頼クライアント認証の負荷の評価

多くの環境では、信頼によって接続された1つ以上のドメインが存在する場合があります。 Kerberos 認証を使用しない別のドメインの id に対する認証要求では、ドメインコントローラーのセキュリティで保護されたチャネルを使用して、宛先ドメインまたは宛先ドメインへのパス内の次のドメインにある別のドメインコントローラーへの信頼をスキャンする必要があります。 信頼される側のドメインのドメインコントローラーに対してドメインコントローラーが行うことができる、セキュリティで保護されたチャネルを使用した同時呼び出しの数は、 **MaxConcurrentAPI**と呼ばれる設定によって制御されます。 ドメインコントローラーでは、セキュリティで保護されたチャネルが負荷の量を処理できることを保証するために、 **MaxConcurrentAPI**のチューニング、またはフォレスト内でのショートカットの信頼の作成という2つの方法があります。 個々の信頼におけるトラフィック量を測定するには、 [MaxConcurrentApi 設定を使用して NTLM 認証のパフォーマンスチューニングを行う方法](https://support.microsoft.com/kb/2688798)を参照してください。

データ収集中は、他のすべてのシナリオと同様に、データが役に立つように、その日のピーク時の時間帯に収集する必要があります。

> [!NOTE]
> フォレスト内およびフォレスト間のシナリオでは、認証が複数の信頼を通過し、各ステージを調整する必要がある場合があります。

#### <a name="planning"></a>計画

NTLM 認証を既定で使用するアプリケーションは多数あります。または、特定の構成シナリオで使用します。 アプリケーションサーバーの容量が増え、アクティブなクライアントの数が増えています。 また、クライアントがセッションを一定期間開いたままにしておき、定期的に再接続する傾向もあります (電子メールプル同期など)。 NTLM の高負荷のもう1つの一般的な例は、インターネットアクセスの認証を必要とする web プロキシサーバーです。

これらのアプリケーションは、特にユーザーとリソースが異なるドメインにある場合に、NTLM 認証に大きな負荷がかかる可能性があります。

クロス信頼負荷を管理する方法は複数ありますが、実際には、/またはシナリオではなく、組み合わせて使用されます。 オプションは次のとおりです。

- ユーザーが常駐しているドメイン内でユーザーが使用しているサービスを特定することによって、信頼度の異なるクライアント認証を減らします。
- 使用可能なセキュリティで保護されたチャネルの数を増やします。 これは、フォレスト間およびフォレスト間のトラフィックに関連し、ショートカットの信頼と呼ばれます。
- **MaxConcurrentAPI**の既定の設定を調整します。

既存のサーバーで**MaxConcurrentAPI**をチューニングする場合、式は次のようになります。

> *New_MaxConcurrentApi_setting* &ge;(*semaphore_acquires*  + *semaphore_time アウト*)&times; *average_semaphore_hold_time* &divide; *time_collection_length*

詳細については、「[サポート技術情報の記事 2688798: MaxConcurrentApi 設定を使用して NTLM 認証のパフォーマンスチューニングを行う方法](https://support.microsoft.com/kb/2688798)」を参照してください。

## <a name="virtualization-considerations"></a>仮想化に関する考慮事項

なし。これは、オペレーティングシステムのチューニング設定です。

### <a name="calculation-summary-example"></a>計算の概要の例

|データ型|値|
|-|-|
|セマフォの取得 (最小)|6161|
|セマフォの取得 (最大)|6762|
|セマフォタイムアウト|0|
|平均セマフォホールド時間|0.012|
|収集期間 (秒)|1:11 分 (71 秒)|
|式 (KB 2688798 から)|((6762 &ndash; 6161) + 0) &times; 0.012/|
|**MaxConcurrentAPI**の最小値|((6762 &ndash; 6161) + 0) &times; 0.012 &divide; 71 =. 101|

この期間のこのシステムでは、既定値が許容されます。

## <a name="monitoring-for-compliance-with-capacity-planning-goals"></a>キャパシティプランニング目標のコンプライアンス対応の監視

この記事では、使用率の目標に向けた計画とスケーリングについて説明しました。 システムが適切な容量のしきい値を超えて動作していることを確認するために監視する必要がある推奨しきい値の概要グラフを次に示します。 これらはパフォーマンスのしきい値ではなく、容量計画のしきい値であることに注意してください。 これらのしきい値を超えて動作しているサーバーは機能しますが、すべてのアプリケーションが正常に動作していることの検証を開始します。 アプリケーションが正常に動作している場合は、ハードウェアのアップグレードやその他の構成の変更の評価を開始します。

|カテゴリ|パフォーマンス カウンター|間隔/サンプリング|移行先|警告|
|-|-|-|-|-|
|プロセッサ|プロセッサ情報 (_Total) \\ % Processor Utility|60 分|40%|60%|
|RAM (Windows Server 2008 R2 以前)|Memory\ 使用可能な MB|< 100 MB|なし|< 100 MB|
|RAM (Windows Server 2012)|メモリ/期間の平均スタンバイキャッシュ有効期間 (秒)|30 分|テストする必要があります|テストする必要があります|
|ネットワーク|ネットワークインターフェイス ( \* ) \ 送信バイト数/秒<p>ネットワークインターフェイス ( \* ) \ 受信バイト数/秒|30 分|40%|60%|
|ストレージ|LogicalDisk ( *\<NTDS Database Drive\>* ) \ Avg Disk sec/Read<p>LogicalDisk ( *\<NTDS Database Drive\>* ) \ ディスクの平均書き込み秒数|60 分|10 ms|15 ms|
|AD サービス|Netlogon ( \* ) \ セマフォの平均保持時間|60 分|0|1 秒|

## <a name="appendix-a-cpu-sizing-criteria"></a>付録 A: CPU のサイズ設定の条件

### <a name="definitions"></a>定義

**プロセッサ (マイクロプロセッサ) –** プログラムの命令を読み込んで実行するコンポーネント

**CPU –** 中央処理ユニット

**マルチコアプロセッサ–** 同じ統合回線上の複数の cpu

**マルチ cpu-** 複数の cpu (同じ統合回線上にありません)

**論理プロセッサ–** オペレーティングシステムの観点から見た1つの論理コンピューティングエンジン

これには、ハイパースレッド、マルチコアプロセッサ上の1つのコア、または1つのコアプロセッサが含まれます。

現在のサーバーシステムには複数のプロセッサ、複数のマルチコアプロセッサ、ハイパースレッドがあるため、この情報は両方のシナリオに対応するために一般化されています。 そのため、論理プロセッサという用語は、使用可能なコンピューティングエンジンのオペレーティングシステムとアプリケーションの観点を表すために使用されます。

### <a name="thread-level-parallelism"></a>スレッドレベルの並列処理

各スレッドには独自のスタックと命令があるため、各スレッドは独立したタスクです。 AD DS はマルチスレッドであり、使用可能なスレッドの数は[Ntdsutil.exeを使用して Active Directory の LDAP ポリシーを表示および設定する方法](https://support.microsoft.com/kb/315071)を使用してチューニングできます。これは、複数の論理プロセッサにわたって適切にスケールされます。

### <a name="data-level-parallelism"></a>データレベルの並列処理

これには、1つのプロセス内の複数のスレッド間でのデータの共有 (AD DS プロセスの場合のみ) と、複数のプロセス内の複数のスレッド間でのデータの共有 (一般に) が含まれます。 この場合、大文字と小文字を区別することを考慮して、データに対するすべての変更が、スレッドを実行しているすべてのコアと共有メモリの更新について、すべてのさまざまなレベルのキャッシュ (L1、L2、L3) のすべての実行中のスレッドに反映されることを意味します。 命令処理を続行する前に、すべてのメモリの場所に一貫性があるため、書き込み操作中にパフォーマンスが低下することがあります。

### <a name="cpu-speed-vs-multiple-core-considerations"></a>CPU 速度とマルチコアに関する考慮事項

一般的な目安として、論理プロセッサの方が高速であるため、一連の命令を処理するのにかかる時間が短縮されますが、論理プロセッサが多くなるほど、タスクを同時に実行できるようになります。 これらの一般的な規則は、共有メモリからデータをフェッチすること、データレベルの並列処理を待機すること、および複数のスレッドを管理するオーバーヘッドを考慮して、本質的に複雑になります。 これは、マルチコアシステムでのスケーラビリティが線形ではない理由でもあります。

これらの考慮事項には次のようなものが考えられます。たとえば、各スレッドが個々の車であり、各レーンがコアであり、クロック速度の速度が上限であるとします。

1. 幹線道路に自動車が1つしかない場合、2つのレーンまたは12レーンがあるかどうかは問題になりません。 その車は、速度の制限によって可能な限り高速に実行されます。
1. スレッドが必要とするデータはすぐには使用できないものとします。 この例えは、道路のセグメントがシャットダウンされるというものです。 幹線道路に自動車が1つしかない場合、lane が再び開かれるまでの速度の制限は関係ありません (データはメモリからフェッチされます)。
1. 自動車の数が増えるにつれて、自動車の数を管理するためのオーバーヘッドが増加します。 運転の経験と必要な注意事項を比較します。これは、道路が事実上空 (深夜の夜など) である場合や、トラフィックの負荷が高い場合 (たとえば、午後1時を除く) にかかる時間です。 また、2つの lane 幹線道路を運転する場合に必要な注意事項を検討します。この場合、運転しているものについては、他の多くのドライバーが何をしているかを心配する必要がある、6レーンの幹線道路は考慮しません。
   > [!NOTE]
   > 急ぎ時間のシナリオについては、次のセクション「応答時間/システム Busyness によるパフォーマンスへの影響」で拡張されています。

その結果、より多くのプロセッサまたはより高速なプロセッサについての詳細は、アプリケーションの動作に対して非常に主観的になります。これは、AD DS の場合、環境固有のものであり、環境内のサーバーごとに異なることもあります。 このため、記事の前半で説明した参照は過度に正確には投資されないため、安全性の余白が計算に含まれています。 予算に基づいた購入の決定を行うときは、プロセッサの使用率を 40% (または環境に適した数) で最適化することをお勧めします。その前に、より高速なプロセッサの購入を検討します。 より多くのプロセッサにわたる同期が増加すると、直線的な進行からより多くのプロセッサの真の利点が低下します (2 &times; プロセッサの数が、 &times; 使用可能な追加のコンピューティング能力を2未満にします)。

> [!NOTE]
> アムダールの法律と Gustafson の法律は、ここで関連する概念です。

### <a name="response-timehow-the-system-busyness-impacts-performance"></a>応答時間/システム busyness がパフォーマンスに与える影響

キュー理論は、待機回線 (キュー) の数学的な調査です。 キュー理論では、使用法は次の式で表されます。

*U* k = *B* &divide; *T*

*U* k は使用率を示し、 *B*はビジー時間の長さ、 *T*はシステムが観察された合計時間です。 これは、Windows のコンテキストに変換されたものです。これは、実行中の状態にある100ナノ秒 (ns) 間隔のスレッド数を、特定の時間間隔で使用できる 100-ns 間隔の数で割った値を意味します。 これは、% Processor Utility (参照[プロセッサオブジェクト](/previous-versions/ms804036(v=msdn.10))と[PERF_100NSEC_TIMER_INV](/previous-versions/windows/embedded/ms901169(v=msdn.10))) を計算するための式です。

また、キュー理論では、 *N*  =  *U* &divide; &ndash; 使用率に基づいて待機中の項目の数を推定する n u k (1 *U* k) という式も用意されています ( *n*はキューの長さです)。 この値をすべての使用率の間隔でグラフ化することで、プロセッサ上で取得するキューが特定の CPU 負荷にどれだけかかるかを推定します。

![Queue length](media/capacity-planning-considerations-queue-length.png)

50% の CPU 負荷が発生すると、平均して、キュー内の他の1つの項目が常に待機状態になることが確認されます。これは、CPU 使用率が約70% になると、非常に迅速に増加します。

このセクションで前に使用した運転の例えに戻ります。

- "午後1時" のビジー時間は、40% から70% の範囲のどこかに仮想的ます。 1つのレーンを選択する機能が majorly に制限されておらず、他のドライバーが高負荷のときには、他の車との間に安全なギャップを "見つける" ことができないようにするために、トラフィックが不足しています。
- トラフィックが1時間の間に近づくと、道路システムは100% の容量に近づいていることがわかります。 車が非常に近いため、そのためには注意が必要であるため、レーンを変更することは非常に困難になる可能性があります。

このため、容量控えめの長期的な平均が40% になると、負荷が急増することがあります。たとえば、急増している一時的なクエリ (数分のうちに実行されるクエリが不十分な場合など)、または通常の負荷で異常なバースト (長期週末後の朝) が発生します。

上のステートメントでは、一般的な閲覧者が簡単にするために、使用法と同じように% Processor Time の計算が同じであると見なされます。 数学的に厳密になります。
- [PERF_100NSEC_TIMER_INV](/previous-versions/windows/embedded/ms901169(v=msdn.10))の変換
  - *B* = 100-ns Interval "Idle" スレッドが論理プロセッサに費やした回数。 [PERF_100NSEC_TIMER_INV](/previous-versions/windows/embedded/ms901169(v=msdn.10))計算での "*X*" 変数の変更
  - *T* = 特定の時間範囲における 100-ns 間隔の合計数。 [PERF_100NSEC_TIMER_INV](/previous-versions/windows/embedded/ms901169(v=msdn.10))計算の "*Y*" 変数の変更。
  - *U* k = "アイドル状態のスレッド" またはアイドル時間の割合による論理プロセッサの使用率。
- 次のように計算します。
  - *U* k = 1-プロセッサ時間の割合
  - % Processor Time = 1 – *U* k
  - % Processor Time = 1 – *B*  /  *T*
  - % Processor Time = 1 – *X1* – *X0*  /  *Y1* – *Y0*

### <a name="applying-the-concepts-to-capacity-planning"></a>容量計画に概念を適用する

前の計算では、システムで必要とされる論理プロセッサの数についての判断が overwhelmingly 複雑に思えるかもしれません。 このため、システムのサイズを変更するアプローチは、現在の負荷に基づいて最大ターゲット使用率を判断し、そこに到達するために必要な論理プロセッサの数を計算することに重点を置いています。 また、論理プロセッサの速度は、パフォーマンス、キャッシュの効率、メモリの一貫性の要件、スレッドのスケジュールと同期、および完全に分散されたクライアントの負荷に大きな影響を与えますが、サーバーごとに異なるパフォーマンスに大きな影響があります。 コンピューティング能力のコストが比較的安いため、必要な Cpu の数を分析して特定することは、ビジネス価値を提供するよりも教育を行うことになります。

40% はハードで高速な要件ではなく、妥当な開始です。 Active Directory のさまざまなコンシューマーは、さまざまなレベルの応答性を必要とします。 プロセッサへのアクセスの待機時間が長くなると、クライアントのパフォーマンスに大きな影響を与えないため、環境が80% または90% の使用率で継続して実行される場合があります。 システムには、RAM へのアクセス、ディスクへのアクセス、ネットワーク経由の応答の送信など、システム内の論理プロセッサよりもはるかに低速な領域が多数存在することを再処理することが重要です。 これらのすべての項目は、組み合わせて調整する必要があります。 例 :

- ディスクにバインドされている90% を実行しているシステムにプロセッサを追加すると、パフォーマンスが大幅に向上しない場合があります。 多くの場合、システムの詳細な分析では、スレッドが完了するのを待機しているので、プロセッサにも多くのスレッドが存在しないことを特定します。
- ディスクにバインドされた問題を解決することは、以前に待機状態で多くの時間を費やしていたスレッドが i/o の待機状態にならなくなり、CPU 時間の競合が増加することを意味します。つまり、前の例の90% の使用率が100% になります (上位にすることはできません)。 両方のコンポーネントを一緒にチューニングする必要があります。
  > [!NOTE]
  > プロセッサ情報 (*) \\ % Processor Utility は、"ターボ" モードのシステムでは100% を超える可能性があります。  これは、CPU が短い期間のプロセッサ速度の評価を超えている場所です。  詳細については、CPU 製造元のドキュメントとカウンターの説明を参照してください。

システム使用率に関するすべての考慮事項についても説明します。 [応答時間/システム busyness がパフォーマンスに与える影響は](#response-timehow-the-system-busyness-impacts-performance)、仮想化されたシナリオではホストとゲストの両方に適用されます。 これは、ゲストが1つしかないホストでは、ドメインコントローラー (および通常はすべてのシステム) が物理ハードウェアで行うパフォーマンスとほぼ同じパフォーマンスを持つためです。 ホストにゲストを追加すると、基になるホストの使用率が高くなり、前に説明したように、待機時間が増加してプロセッサにアクセスできるようになります。 つまり、論理プロセッサの使用率は、ホストとゲストレベルの両方で管理する必要があります。

前の比喩を拡張して、物理ハードウェアとして幹線道路を残しておくと、ゲスト VM はバス (rider が希望する宛先に直接移動する express bus) と analogized されます。 次の4つのシナリオを考えてみましょう。

- 時間が空いていて、rider がバス上でほぼ空である場合、バスもほぼ空の道路で取得します。 競合するトラフィックがないため、rider は、rider が代わりに使用されているかのように簡単に行うことができます。 Rider の移動時間は、依然として速度の制限によって制限されます。
- バスはほとんど空ですが、道路のレーンのほとんどは閉じているため、幹線道路はまだ混雑していません。 Rider は、混雑している道路のほぼ空のバス上にあります。 Rider は、どこに座っているかについては、バスに多数のコンテストを持ちませんが、外部のトラフィックの残りの部分では、合計旅行時間が変わります。
- 幹線道路とバスが混雑しています。 旅行の時間が長くなるだけでなく、バスをオンまたはオフにするのは悪夢としています。 バス (ゲストに論理プロセッサ) を追加すると、それらがより簡単に道路に適合するという意味ではなく、旅行が短縮されます。
- 最後のシナリオでは、少しずつ拡大しているかもしれませんが、バスはいっぱいになりますが、道路は混雑していません。 Rider はバスのオンとオフに問題がありますが、バスが外出した後に、トリップが効率的になります。 これは、より多くのバス (ゲストに論理プロセッサ) を追加すると、ゲストのパフォーマンスが向上する唯一のシナリオです。

そこから、0% が使用されている場合と、道路の100% が使用されている状態の間に、さまざまな影響を与えるバスの0% と100% の使用状態の間に、いくつかのシナリオがあることを比較的簡単に推定できます。

40% CPU の上位にあるプリンシパルをホストとゲストの妥当なターゲットとして適用することは、上記のキューの量と同じ理由から合理的に開始されます。

## <a name="appendix-b-considerations-regarding-different-processor-speeds-and-the-effect-of-processor-power-management-on-processor-speeds"></a>付録 B: さまざまなプロセッサ速度に関する考慮事項とプロセッサ速度によるプロセッサ電源管理の影響

プロセッサの選択に関するセクションでは、プロセッサがクロック速度の100% で実行されていることを前提としています。データが収集されていて、代替システムの速度が同じであることが前提となっています。 実際には、特に Windows Server 2008 R2 以降 (特に、既定の電源プランが**均衡**) であることを前提としていますが、この方法は保守的なアプローチであるという意味でもあります。 潜在的なエラー率が増加する可能性はありますが、プロセッサの速度が上がるにつれて安全性の余白が増加するだけです。

- たとえば、11.25 の Cpu が必要なシナリオで、データが収集された速度でプロセッサが実行されていた場合、より正確な推定は 5.125 2 になる可能性があり &divide; ます。
- クロック速度を2倍にすると、特定の期間に発生する処理の量が2倍になることを保証することはできません。 これは、RAM またはその他のシステムコンポーネントを待機しているプロセッサの時間が同じままになる可能性があるためです。 実質的な効果として、より高速なプロセッサでは、データのフェッチを待機している間、アイドル時間の割合が高くなる可能性があります。 ここでも、プロセッサ速度の線形比較を想定することで、最も低い共通の分母を維持し、控えめなものにして、可能性のある偽レベルの精度を計算しないようにすることをお勧めします。

代替ハードウェアのプロセッサ速度が現在のハードウェアより低い場合は、過剰な量によって必要とされるプロセッサの推定量を増やすことが安全です。 たとえば、サイトの負荷を維持するために10個のプロセッサが必要であり、現在のプロセッサが 3.3 Ghz で実行されており、交換用プロセッサが 2.6 Ghz で動作することが計算されます。これは、速度が21% 低下することを示します。 この場合、12個のプロセッサが推奨される量になります。

ただし、このような変動によって、容量管理のプロセッサ使用率のターゲットが変更されることはありません。 プロセッサのクロック速度は、必要な負荷に基づいて動的に調整されるため、高い負荷でシステムを実行すると、CPU がクロック速度の高い状態でより多くの時間を費やし、最終的な目標が100% のクロック速度のピーク状態で40% の使用率になるというシナリオが生成されます。 CPU 速度は、ピーク時以外のシナリオでは調整されるので、これよりも小さいものがあるとしても、省電力が生成されます。

> [!NOTE]
> データが収集されている間、プロセッサの電源管理をオフにする (電源プランを**高パフォーマンス**に設定する) ことができます。 これにより、対象サーバーでの CPU 消費量がより正確に表現されます。

異なるプロセッサの見積もりを調整するために、上記で説明した他のシステムのボトルネックを除外することで、プロセッサの速度を2倍にして、実行可能な処理量を倍増させることを前提としています。  現在、プロセッサの内部アーキテクチャはプロセッサ間で十分に異なります。これは、標準のパフォーマンス評価 Corporation の SPECint_rate2006 ベンチマークを利用することで、異なるプロセッサを使用した場合と比較した場合よりも、より安全な方法で測定できます。

1. 使用中で、使用する予定のプロセッサの SPECint_rate2006 スコアを確認します。
    1. 標準のパフォーマンス評価 Corporation の web サイトで、[ **results**] を選択し、[ **CPU2006**] を強調表示して、[**すべての SPECint_rate2006 の結果を検索**] を選択します。
    1. [ **Simple Request**] \ (簡易要求 \) で、ターゲットプロセッサの検索条件を入力します。たとえば、**プロセッサは e5-2630 (Baselinetarget) に一致**し、**プロセッサは e5-2650 (ベースライン) に一致**します。
    1. 使用するサーバーとプロセッサの構成 (完全に一致するものがない場合は閉じるもの) を見つけて、[**結果**] 列と [ **# コア**] 列の値を確認します。
1. 修飾子を決定するには、次の式を使用します。
   >(*コアスコア値あたりのターゲットプラットフォーム*) &times;(*ベースラインプラットフォームのコアあたり MHz*)&divide;((*コアスコア値ごとの基準*) &times; (*ターゲットプラットフォームのコアあたり MHz*)

    上記の例を使用します。
   >(35.83 &times; 2000) &divide; (33.75 &times; 2300) = 0.92
1. 修飾子によってプロセッサの推定数を乗算します。  上記のケースでは、E5-2650 プロセッサから E5-2630 プロセッサにアクセスするために、計算された 11.25 Cpu &times; 0.92 = 10.35 プロセッサが必要です。

## <a name="appendix-c-fundamentals-regarding-the-operating-system-interacting-with-storage"></a>付録 C: 記憶域と対話するオペレーティングシステムに関する基礎

「[応答時間/システム busyness によるパフォーマンスへの影響](#response-timehow-the-system-busyness-impacts-performance)」で説明されているキュー理論の概念は、記憶域にも適用できます。 これらの概念を適用するには、オペレーティングシステムが i/o を処理する方法を理解しておく必要があります。 Microsoft Windows オペレーティングシステムでは、i/o 要求を保持するためのキューが物理ディスクごとに作成されます。 ただし、物理ディスクを明確にする必要があります。 配列コントローラーと San は、1つの物理ディスクとして、オペレーティングシステムにスピンドルの集計を提示します。 さらに、配列コントローラーと San では、複数のディスクを1つの配列セットに集約し、この配列セットを複数の "パーティション" に分割することができます。これは、複数の物理ディスク (参照図) としてオペレーティングシステムに提示されます。

![ブロックスピンドル](media/capacity-planning-considerations-block-spindles.png)

この図では、2つのスピンドルがミラー化され、データストレージ (Data 1 と Data 2) の論理領域に分割されています。 これらの論理領域は、オペレーティングシステムによって別々の物理ディスクとして表示されます。

これは非常に複雑になる可能性がありますが、この付録では、さまざまなエンティティを識別するために次の用語が使用されています。

- **スピンドル–** サーバーに物理的に取り付けられているデバイス。
- **Array –** コントローラーによって集計されたスピンドルのコレクションです。
- **配列パーティション–** 集計配列のパーティション分割
- **LUN –** san を参照するときに使用される配列
- **ディスク-** オペレーティングシステムは、単一の物理ディスクとして使用します。
- **パーティション–** オペレーティングシステムが物理ディスクとして認識する内容を論理的にパーティション分割します。

### <a name="operating-system-architecture-considerations"></a>オペレーティングシステムのアーキテクチャに関する考慮事項

オペレーティングシステムは、監視される各ディスクに対して先入れ先出し (FIFO) の i/o キューを作成します。このディスクは、スピンドル、配列、または配列パーティションを表している可能性があります。 オペレーティングシステムの観点から、i/o の処理に関しては、よりアクティブなキューの方が優れています。 FIFO キューはシリアル化されます。つまり、ストレージサブシステムに対して発行されるすべての i/o は、要求が到着した順序で処理される必要があります。 オペレーティングシステムによって検出された各ディスクをスピンドル/アレイに関連付けることにより、オペレーティングシステムは、ディスク全体の i/o キューを管理するようになりました。これにより、ディスク間の大量の i/o リソースの競合が解消され、i/o 要求が1つのディスクに分離されます。 例外として、Windows Server 2008 では i/o 優先順位付けの概念が導入されており、"低" 優先順位を使用するように設計されたアプリケーションは、この通常の順序から、バック席を取得します。 "低" 優先順位の既定値を "通常" に利用するようにコード化されていないアプリケーション

### <a name="introducing-simple-storage-subsystems"></a>単純な記憶域サブシステムの概要

単純な例 (コンピューター内の1つのハードドライブ) から、コンポーネントごとの分析が提供されます。 次のように、システムを主要なストレージサブシステムコンポーネントに分割します。

- **1 ~** 1万 RPM 超高速 scsi HD (ウルトラ高速 scsi には 20 MB/秒の転送速度があります)
- **1 –** SCSI バス (ケーブル)
- **1 –** 超高速 SCSI アダプター
- **1 –** 32 ビット 33 MHz PCI バス

コンポーネントが特定されたら、システムを転送できるデータの量、または処理できる i/o の量を計算できます。 システムを転送できる i/o とデータの量は相互に関連付けられていますが、同じではありません。 この相関関係は、ディスク i/o がランダムか順次か、ブロックサイズによって決まります。 (すべてのデータはブロックとしてディスクに書き込まれますが、異なるブロックサイズを使用するアプリケーションは異なります)。コンポーネントごとに、次のようにします。

- **ハードドライブ-** 平均 1万 RPM ハードドライブには、7ミリ秒 (ms) のシーク時間と3ミリ秒のアクセス時間があります。 シーク時間は、読み取り/書き込みヘッドがプラッタ上の場所に移動するのにかかる平均時間です。 アクセス時間は、ヘッドが適切な場所にある場合に、ディスクに対するデータの読み取りまたは書き込みにかかる時間の平均値です。 このため、1万 RPM HD で一意のデータブロックを読み取る平均時間は、データのブロックごとに、合計で約10ミリ秒 (.010 秒) のシークとアクセスを構成します。

  すべてのディスクアクセスで、ヘッドをディスク上の新しい場所に移動する必要がある場合、読み取り/書き込み動作は "ランダム" と呼ばれます。 このため、すべての i/o がランダムである場合、1万 RPM の HD は約 100 i/o per second (IOPS) を処理できます (1 秒あたりの1000ミリ秒、i/o あたり10ミリ秒、1000/10 = 100 IOPS)。

  また、すべての i/o が HD 上の隣接するセクターから発生した場合、これは順次 i/o と呼ばれます。 順次 i/o では、最初の i/o が完了すると、読み取り/書き込みヘッドが HD に格納される次のデータブロックの先頭にあるため、シーク時間はありません。 したがって、1万 RPM の HD は、1秒あたり約 333 i/o を処理できます (1 秒あたり1000ミリ秒、i/o あたり3ミリ秒で割ったもの)。

  > [!NOTE]
  > この例では、ディスクキャッシュは反映されません。この場合、1つのシリンダーのデータは通常保持されます。 この場合、最初の i/o には10ミリ秒が必要であり、ディスクはシリンダー全体を読み取ります。 その他のすべての順次 i/o はキャッシュから満たされます。 その結果、ディスク内キャッシュは順次 i/o パフォーマンスを向上させる可能性があります。

  これまでは、ハードドライブの転送速度には関係がありませんでした。 ハードドライブが 20 MB/s ウルトラワイドであるか、Ultra3 160 MB/秒であるかにかかわらず、が 1万 RPM HD によって処理される実際の IOPS の量は、約100ランダムまたは ~ 300 シーケンシャル i/o です。 ドライブに書き込むアプリケーションに基づいてブロックサイズが変化すると、i/o ごとにプルされるデータの量が異なります。 たとえば、ブロックサイズが 8 KB の場合、100の i/o 操作では、ハードドライブの合計が 800 KB になります。 ただし、ブロックサイズが 32 KB の場合、100の i/o では、ハードドライブに対して 3200 KB (3.2 MB) の読み取り/書き込みが行われます。 SCSI 転送速度が、転送されたデータ量の合計を超えている限り、"高速な" 転送速度のドライブを取得すると、何も得られません。 比較については、次の表を参照してください。

  | |7200 RPM 9ms seek、4ミリ秒 access|1万 RPM 7ms seek、3ミリ秒 access|15000 RPM 4 ミリ秒 seek、2ミリ秒 access
  |-|-|-|-|
  |ランダム I/O|80|100|150|
  |順次 i/o|250|300|500|

  |1万 RPM ドライブ|8 KB ブロックサイズ (Active Directory Jet)|
  |-|-|
  |ランダム I/O|800 KB/秒|
  |順次 i/o|2400 KB/秒|

- **SCSI バックプレーン (バス) –**「SCSI バックプレーン (バス)」、またはこのシナリオでは、リボンケーブルがストレージサブシステムのスループットに与える影響を理解することは、ブロックサイズに関する知識に依存します。 基本的に、i/o が 8 KB のブロックにある場合、バスはどのくらいの i/o を処理できますか。 このシナリオでは、SCSI バスは 20 MB/秒、つまり 20480 KB/秒です。 20480 KB/秒を 8 KB ブロックで割ると、SCSI バスでサポートされる最大 2500 IOPS が得られます。

  > [!NOTE]
  > 次の表に示す図は、例を示しています。 接続されているほとんどのストレージデバイスは現在 PCI Express を使用しています。これにより、スループットが大幅に向上します。

  |SCSI バスでブロックサイズごとにサポートされる i/o|2 KB ブロックサイズ|8 KB ブロックサイズ (AD Jet) (SQL Server 7.0/SQL Server 2000)
  |-|-|-|
  |20 MB/秒|10,000|2,500|
  |40 MB/秒|20,000|5,000|
  |128 MB/秒|65,536|16,384|
  |320 MB/秒|160,000|40,000|

  このグラフから判断できるように、どのような用途でも、バスはボトルネックになることはありません。これは、スピンドルの最大値が 100 i/o で、上記のいずれかのしきい値を下回っているためです。

  > [!NOTE]
  > これは、SCSI バスが100% 効率的であることを前提としています。

- **SCSI アダプター–** こので処理できる i/o の量を決定するには、製造元の仕様を確認する必要があります。 I/o 要求を適切なデバイスに転送するには、何らかの種類の処理が必要です。したがって、処理可能な i/o の量は SCSI アダプター (または配列コントローラー) プロセッサに依存します。

  この例では、1000 i/o を処理できることを前提としています。

- **PCI バス–** これは、しばしば見落とされるコンポーネントです。 この例では、これはボトルネックにはなりません。ただし、システムがスケールアップされると、ボトルネックになる可能性があります。 参考までに、33Mhz で動作する32ビット PCI バスは、理論上は 133 MB/秒のデータを転送できます。 次に式を示します。
  > 32ビット &divide; 8 ビット/バイト &times; 33 MHz = 133 MB/秒。

  は理論的な制限であることに注意してください。実際、特定のバーストシナリオでは、約50% の最大値にしか到達できませんが、短い期間は75% の効率を得ることができます。

  66Mhz 64 ビット PCI バスは、理論的な最大値 (64 ビット/ &divide; バイト &times; 66 Mhz) = 528 MB/秒をサポートできます。また、その他のデバイス (ネットワークアダプター、2番目の SCSI コントローラーなど) で帯域幅が共有され、デバイスが制限されたリソースに対して競合するため、使用可能な帯域幅が減少します。

この記憶域サブシステムのコンポーネントを分析した後、スピンドルは要求可能な i/o の量の制限要因であり、その結果、システムを転送できるデータの量が決まります。 具体的には、AD DS のシナリオでは、これは100のランダム i/o (1 秒あたりのランダム i/o 数) は 8 KB 単位で行われ、Jet データベースにアクセスすると、合計 800 KB/秒になります。 また、ログファイルに排他的に割り当てられているスピンドルの最大スループットには、300の制限があります。1秒あたりの1秒あたりの順次 i/o 2.4 (8 KB 2400 単位)。

ここでは、単純な構成を分析した後、記憶域サブシステムのコンポーネントが変更または追加されるとボトルネックが発生する場所を次の表に示します。

|Notes|ボトルネック分析|ディスク|バス型|アダプター|PCI バス|
|-|-|-|-|-|-|
|これは、2番目のディスクを追加した後のドメインコントローラー構成です。 ディスク構成は、800 KB/秒のボトルネックを表します。|1ディスクの追加 (合計 = 2)<p>I/o はランダムです。<p>4 KB ブロックサイズ<p>1万 RPM HD|200 i/o 合計<br />800 KB/秒の合計。| | | |
|7台のディスクを追加した後も、ディスク構成は 3200 KB/秒のボトルネックを示します。|**7ディスクの追加 (合計 = 8)**  <p>I/o はランダムです。<p>4 KB ブロックサイズ<p>1万 RPM HD|800 i/o 合計。<br />3200 KB/秒の合計| | | |
|I/o を順次に変更すると、ネットワークアダプターは 1000 IOPS に制限されるため、ボトルネックになります。|7ディスクの追加 (合計 = 8)<p>**I/o はシーケンシャルです。**<p>4 KB ブロックサイズ<p>1万 RPM HD| | |2400 i/o 秒はディスクに対して読み取り/書き込みが可能で、コントローラーは 1000 IOPS に制限されています。| |
|ネットワークアダプターを 1万 IOPS をサポートする SCSI アダプターに置き換えると、ボトルネックがディスク構成に戻ります。|7ディスクの追加 (合計 = 8)<p>I/o はランダムです。<p>4 KB ブロックサイズ<p>1万 RPM HD<p>**SCSI アダプターのアップグレード (1万 i/o がサポートされるようになりました)**|800 i/o 合計。<br />3200 KB/秒の合計| | | |
|ブロックサイズを 32 KB に増やした後、バスは 20 MB/秒しかサポートしていないため、ボトルネックになります。|7ディスクの追加 (合計 = 8)<p>I/o はランダムです。<p>**32 KB ブロックサイズ**<p>1万 RPM HD| |800 i/o 合計。 25600 KB/秒 (25 MB/秒) をディスクに読み取り/書き込みできます。<p>バスは 20 MB/s のみをサポートします。| | |
|バスをアップグレードし、ディスクを追加すると、ディスクがボトルネックのままになります。|**13個のディスク (合計 = 14) の追加**<p>14台のディスクがある2つ目の SCSI アダプターを追加する<p>I/o はランダムです。<p>4 KB ブロックサイズ<p>1万 RPM HD<p>**320 MB/s SCSI バスへのアップグレード**|2800 i/o<p>11200 KB/秒 (10.9 MB/秒)| | | |
|I/o を順次に変更すると、ディスクはボトルネックのままになります。|13個のディスク (合計 = 14) の追加<p>14台のディスクがある2つ目の SCSI アダプターを追加する<p>**I/o はシーケンシャルです。**<p>4 KB ブロックサイズ<p>1万 RPM HD<p>320 MB/s SCSI バスへのアップグレード|8400 i/o<p>33600 KB\s<p>(32.8 MB\s)| | | |
|より高速なハードドライブを追加した後、ディスクはボトルネックのままです。|13個のディスク (合計 = 14) の追加<p>14台のディスクがある2つ目の SCSI アダプターを追加する<p>I/o はシーケンシャルです。<p>4 KB ブロックサイズ<p>**15000 RPM HD**<p>320 MB/s SCSI バスへのアップグレード|14000 i/o<p>56000 KB/秒<p>(54.7 MB/秒)| | | |
|ブロックサイズを 32 KB に増やすと、PCI バスがボトルネックになります。|13個のディスク (合計 = 14) の追加<p>14台のディスクがある2つ目の SCSI アダプターを追加する<p>I/o はシーケンシャルです。<p>**32 KB ブロックサイズ**<p>15000 RPM HD<p>320 MB/s SCSI バスへのアップグレード| | | |14000 i/o<p>448000 KB/秒<p>(437 MB/秒) は、スピンドルに対する読み取り/書き込みの上限です。<p>PCI バスでは、理論上最大 133 MB/秒 (最適な75% の効率) がサポートされています。|

### <a name="introducing-raid"></a>RAID の導入

配列コントローラーが導入されると、記憶域サブシステムの特性は大きく変わりません。これは、計算で SCSI アダプターを置き換えるだけです。 変更点は、さまざまなアレイレベル (RAID 0、RAID 1、RAID 5 など) を使用する場合に、ディスクへのデータの読み取りと書き込みを行うコストです。

RAID 0 では、データは RAID セット内のすべてのディスクにわたってストライピングされます。 これは、読み取りまたは書き込み操作中に、データの一部が各ディスクからプルまたはプッシュされ、同じ期間にシステムを転送できるデータ量が増加することを意味します。 したがって、1秒あたりに各スピンドルで (1万 RPM のドライブを想定して)、100の i/o 操作を実行できます。 サポート可能な i/o の合計量は、スピンドルあたり1秒あたり N スピンドル時間 100 i/o (1 秒あたり 100 * N i/o) です。

![論理 d: ドライブ](media/capacity-planning-considerations-logical-d-drive.png)

RAID 1 では、冗長性を確保するためにスピンドルのペア間でデータがミラー化 (複製) されます。 このため、読み取り i/o 操作を実行すると、セット内の両方のスピンドルからデータを読み取ることができます。 これにより、読み取り操作中に、両方のディスクの i/o 容量を効果的に使用できるようになります。 注意すべき点は、書き込み操作では RAID 1 のパフォーマンス上の利点が得られないことです。 これは、冗長性を確保するために、同じデータを両方のドライブに書き込む必要があるためです。 データの書き込みは両方のスピンドルで同時に行われるので、これは時間がかかりませんが、両方のスピンドルでデータの複製が行われるため、書き込み i/o 操作によって、2つの読み取り操作が行われなくなります。 したがって、すべての書き込み i/o には、2つの読み取り i/o がかかります。 この情報から数式を作成して、発生している i/o 操作の合計数を確認できます。

> *読み取り i/o* + 2 &times; *書き込み i/o*の  =  *合計使用可能なディスク i/o 数*

書き込みの読み取りとスピンドルの数の比率がわかっている場合は、上記の式から次の式を派生させることで、配列でサポートできる最大 i/o 数を特定できます。

> スピンドルあたりの*最大 IOPS* &times;2スピンドル &times; [(*% 読み取り*/  +  *書き込み*) &divide; (*% 読み取り*+ 2 &times; *% 書き込み*)] =*合計 IOPS*

RAID 1 + 0 は、読み取りと書き込みのコストに関して、RAID 1 とまったく同じように動作します。 ただし、i/o はミラー化されたセットごとにストライピングされるようになりました。 状況

> スピンドルあたりの*最大 IOPS* &times;2つのスピンドル &times; [(*% 読み取り*/  +  *書き込み*) &divide; (*% 読み取り*+ 2 &times; *% の書き込み*)] =*合計 i/o 数*

raid 1 セットでは、RAID 1 セットの多重度 (*N*) がストライピングされている場合、処理できる I/o の合計は、 &times; raid 1 セットあたり N i/o になります。

> *N* &times; スピンドル2スピンドル*あたり最大 iops* &times; &times; [(*%* 読み取り  +  *% 書き込み*) &divide; (%*読み取り*+ 2 &times; *% 書き込み*)]} =*合計 iops*

RAID 5 では、 *n* + 1 raid と呼ばれることもありますが、データは*n*スピンドル間でストライピングされ、パリティ情報は "+ 1" スピンドルに書き込まれます。 ただし、raid 5 では、RAID 1 または 1 + 0 よりも書き込み i/o を実行すると、コストが大幅に高くなります。 RAID 5 では、書き込み i/o が配列に送信されるたびに、次のプロセスが実行されます。

1. 古いデータを読み取る
1. 古いパリティを読み取る
1. 新しいデータを書き込む
1. 新しいパリティを書き込む

オペレーティングシステムによって配列コントローラーに送信されるすべての書き込み i/o 要求は、4つの i/o 操作を完了する必要があるため、送信された書き込み要求は、1回の読み取り i/o として完了するまでに4回かかります。 オペレーティングシステムの観点からの i/o 要求を、スピンドルによって見たものに変換する数式を派生させるには、次のようにします。

> *読み取り i/o* + 4 &times; *書き込み i/o*の  =  *合計 i/o 数*

同様に、RAID 1 セットでは、書き込みの読み取りとスピンドルの数の比率がわかっている場合、上記の式から次の式を取得して、配列でサポートできる最大 i/o 数を特定できます (スピンドルの合計数には、パリティに失われた "ドライブ" が含まれていないことに注意してください)。

> スピンドルあたりの*IOPS* &times;(*スピンドル*– 1) &times;[(*% 読み取り*  +  *% 書き込み*) &divide; (%*読み取り*+ 4 &times; *% 書き込み*)] =*合計 IOPS*

### <a name="introducing-sans"></a>San の概要

記憶域サブシステムの複雑さを拡大すると、SAN が環境に導入されるときに、概要を説明する基本的な原則は変わりません。ただし、SAN に接続されているすべてのシステムの i/o 動作は考慮する必要があります。 SAN を使用する場合の大きな利点の1つは、内部または外部に接続されたストレージに対する追加の冗長性です。容量計画では、フォールトトレランスのニーズを考慮する必要があります。 また、評価する必要があるコンポーネントが追加されています。 次のように、SAN をコンポーネントパーツに分割します。

- SCSI またはファイバーチャネルハードドライブ
- ストレージユニットチャネルのバックプレーン
- ストレージ ユニット
- 記憶域コントローラーモジュール
- SAN スイッチ
- HBA
- PCI バス

冗長性を確保するためにシステムを設計する場合は、障害の可能性に対応するために追加のコンポーネントが含まれます。 容量計画時には、冗長コンポーネントを使用可能なリソースから除外することが非常に重要です。 たとえば、SAN に2つのコントローラーモジュールがある場合、1つのコントローラーモジュールの i/o 容量はすべて、システムで使用可能な合計 i/o スループットに使用する必要があります。 これは、1つのコントローラーで障害が発生した場合に、接続されているすべてのシステムに必要な i/o 負荷全体が残りのコントローラーによって処理される必要があるためです。 すべての容量計画がピーク時の使用期間に対して行われるため、冗長コンポーネントを使用可能なリソースに考慮することはできません。また、計画されたピーク使用率はシステムの80% の飽和を超えないようにする必要があります (バーストや異常なシステム動作に対応するため)。 同様に、冗長 SAN スイッチ、ストレージユニット、およびスピンドルは、i/o の計算に考慮するべきではありません。

SCSI またはファイバーチャネルハードドライブの動作を分析する場合、前述のように動作を分析する方法は変更されません。 各プロトコルにはいくつかの利点と欠点がありますが、ディスクごとの制限要因として、ハードドライブの機械的な制限があります。

記憶域ユニットのチャネルの分析は、SCSI バスで使用可能なリソースの計算、または (20 MB/s などの) 帯域幅をブロックサイズ (8 KB など) で割ったものとまったく同じです。 これは、単純な前の例とは、複数のチャネルを集計したものです。 たとえば、6つのチャネルがあり、それぞれが 20 MB/秒の最大転送速度をサポートしていて、使用可能な i/o およびデータ転送の合計量が 100 MB/秒である (これは正しい、120 MB/秒ではありません)。 ここでも、フォールトトレランスは、この計算の主要なプレーヤーです。チャネル全体が失われた場合、システムは5つの機能しているチャネルに残されます。 したがって、障害発生時のパフォーマンスの期待値を継続的に維持するために、すべてのストレージチャネルの合計スループットが 100 MB/秒を超えないようにする必要があります (これは、負荷とフォールトトレランスがすべてのチャネルに均等に分散されることを前提としています)。 これを i/o プロファイルにすることは、アプリケーションの動作に依存します。 Active Directory Jet i/o の場合、これは1秒あたり約 12500 i/o (i/o あたり 100 MB/s 8 KB) に関連付けら &divide; れます。

次に、各モジュールがサポートできるスループットについて理解を深めるために、コントローラーモジュールの製造元の仕様を取得する必要があります。 この例では、SAN にはそれぞれ 7500 i/o をサポートする2つのコントローラーモジュールがあります。 冗長性が望ましくない場合、システムの合計スループットは 15000 IOPS になります。 障害が発生した場合の最大スループットの計算では、1つのコントローラー (または 7500 IOPS) のスループットが制限されます。 このしきい値は、すべてのストレージチャネルでサポートできる最大 12500 IOPS (4 KB のブロックサイズを想定) よりも低いため、現在は分析のボトルネックになっています。 計画のために、計画する必要のある最大 i/o 数は 10400 i/o になります。

データがコントローラーモジュールを終了すると、1 GB/秒 (1 ギガビット/秒) で評価されたファイバーチャネル接続を経由します。 これを他のメトリックと関連付けるために、1 GB/秒は 128 MB/秒 (1 GB/秒 &divide; 8 ビット/バイト) に変わります。 これは、記憶域ユニット (100 MB/秒) のすべてのチャネルで合計帯域幅を超えているため、システムのボトルネックになることはありません。 また、これは2つのチャネルのうちの1つ (冗長性のために追加された 1 GB/秒ファイバーチャネル接続) の1つであるため、1つの接続に失敗した場合、残りの接続には、要求されたすべてのデータ転送を処理するのに十分な容量が残っています。

サーバーにルーティングする場合、データはほとんどの場合、SAN スイッチを転送します。 SAN スイッチでは、着信 i/o 要求を処理し、適切なポートに転送する必要があるため、スイッチには処理可能な i/o の量に制限がありますが、その制限を決定するために製造元の仕様が必要になります。 たとえば、スイッチが2つあり、各スイッチが 1万 IOPS を処理できる場合、合計スループットは 2万 IOPS になります。 この場合も、フォールトトレランスが問題になります。1つのスイッチで障害が発生した場合、システムの合計スループットは 1万 IOPS になります。 通常の操作では80% の使用率を超えないようにする必要があるため、8000の i/o を使用する場合は、ターゲットにする必要があります。

最後に、サーバーにインストールされている HBA にも、処理できる i/o の量に制限があります。 通常、2番目の HBA は冗長性を確保するためにインストールされますが、SAN スイッチの場合と同様に、処理可能な最大 i/o を計算するときに、 *N* 1 hba のスループットの合計が &ndash; システムの最大のスケーラビリティと同じになります。

### <a name="caching-considerations"></a>キャッシュに関する考慮事項

キャッシュは、ストレージシステム内の任意の時点での全体的なパフォーマンスに大きな影響を与える可能性のあるコンポーネントの1つです。 キャッシュアルゴリズムに関する詳細な分析については、この記事では説明しません。ただし、ディスクサブシステムでのキャッシュに関するいくつかの基本的なステートメントは、次の点に注意してください。

- キャッシュを使用すると、連続的なシーケンシャル書き込み i/o が向上します。これは、サイズの小さい書き込み操作を大量の i/o ブロックに格納し、ストレージへのステージングをより少ないで、より大きなブロックサイズで行うことができるためです。 これにより、合計ランダム i/o と合計順次 i/o が削減されるため、他の i/o に対するリソースの可用性が向上します。
- キャッシュでは、ストレージサブシステムの継続的な書き込み i/o スループットは向上しません。 データをコミットするためにスピンドルを使用できるようになるまで、書き込みをバッファリングすることはできません。 記憶域サブシステム内のスピンドルの使用可能なすべての i/o が長時間にわたって飽和状態になると、キャッシュが最終的にいっぱいになります。 キャッシュを空にするには、キャッシュのフラッシュを可能にするのに十分な i/o を提供するために、キャッシュを空にするために十分な時間を確保する必要があります。

  より大きなキャッシュでは、より多くのデータのバッファーが許可されます。 これは、より長い期間の鮮やかさを実現できることを意味します。

  通常の運用記憶域サブシステムでは、データをキャッシュに書き込む必要があるだけなので、オペレーティングシステムの書き込みパフォーマンスが向上します。 基になるメディアが i/o で飽和状態になると、キャッシュがいっぱいになり、書き込みパフォーマンスがディスク速度に戻ります。

- 読み取り i/o をキャッシュする場合、キャッシュが最も便利なシナリオは、データがディスクに順番に格納され、キャッシュが先読みできる場合です (次のセクターに、次に要求されるデータが含まれていることが前提となります)。
- 読み取り i/o がランダムである場合、ドライブコントローラーでのキャッシュは、ディスクから読み取ることができるデータの量に対する拡張機能を提供することはほとんどありません。 オペレーティングシステムまたはアプリケーションベースのキャッシュサイズがハードウェアベースのキャッシュサイズよりも大きい場合、拡張機能は存在しません。

  Active Directory の場合、キャッシュは RAM の容量によってのみ制限されます。

### <a name="ssd-considerations"></a>SSD に関する考慮事項

Ssd は、スピンドルベースのハードディスクとはまったく異なる動物です。 ただし、2つの重要な条件は "処理できる IOPS の数" です。 「このような IOPS の待機時間とは」 スピンドルベースのハードディスクと比較して、Ssd はより大きな i/o を処理でき、待機時間が短くなることがあります。 一般に、この記事の執筆時点では、Ssd はギガバイト単位の比較でコストが高くなりますが、i/o ごとのコストが非常に低くなり、記憶域のパフォーマンスの面で大きな考慮事項になります。

考慮事項:

- IOPS と待機時間はどちらも製造元の設計に対して非常に主観的であり、場合によっては、スピンドルベースのテクノロジよりもパフォーマンスの低下が見られます。 要するに、製造元の仕様のドライブを確認して検証し、generalities を想定していないことが重要です。
- IOPS 型の値は、読み取りまたは書き込みのどちらであるかによって大きく異なる場合があります。 AD DS サービス (一般に、主に読み取りベース) は、他のアプリケーションシナリオよりも影響が少なくなります。
- "耐久性のある書き込み" –これは、SSD のセルが最終的に摩耗する概念です。さまざまな製造元がこのチャレンジの fashions を処理します。 少なくともデータベースドライブでは、主に読み取り i/o プロファイルを使用することで、データが非常に揮発性ではないため、この問題の重要度を高めることができます。

### <a name="summary"></a>まとめ

ストレージについて考えてみる1つの方法は、図示の家庭での組み込みです。 データが格納されているメディアの IOPS が、メインの消耗を想定しているとします。 これが目になる (パイプ内のルートなど)、または制限されている (つまり、折りたたまれている、または小さすぎる) 場合、家庭内のすべてのシンクが過剰に使用されている場合 (ゲスト数が多すぎる場合)。 これは、1つまたは複数のシステムが、同じ基になるメディアを持つ SAN/NAS/iSCSI 上の共有記憶域を利用している共有環境に似ています。 さまざまなシナリオを解決するために、さまざまなアプローチを取ることができます。

- 折りたたまれた、または小さすぎるドレインを行うには、フルスケールの置換と修正が必要です。 これは、新しいハードウェアの追加や、インフラストラクチャ全体の共有記憶域を使用したシステムの再配布に似ています。
- "目になる" パイプは、通常、問題の1つ以上の問題を識別し、それらの問題を除去することを意味します。 記憶域のシナリオでは、ストレージまたはシステムレベルのバックアップ、すべてのサーバーでの同期されたウイルススキャン、およびピーク期間中に実行される同期された最適化ソフトウェアを使用できます。

どのような組み込みの設計でも、複数のドレインがメインのドレインに入ります。 これらのドレインまたは接合ポイントのいずれかが停止した場合、その接合点の背後にあるものだけがバックアップされます。 記憶域のシナリオでは、これは、オーバーロードされたスイッチ (SAN/NAS/iSCSI シナリオ)、ドライバーの互換性の問題 (間違ったドライバー/HBA ファームウェア/storport.sys の組み合わせ)、またはバックアップ/ウイルス対策/最適化です。 ストレージ "パイプ" の大きさが十分であるかどうかを判断するには、IOPS と i/o サイズを測定する必要があります。 各ジョイントで、適切な "パイプの直径" を確保するために、それらをまとめて追加します。

## <a name="appendix-d---discussion-on-storage-troubleshooting---environments-where-providing-at-least-as-much-ram-as-the-database-size-is-not-a-viable-option"></a>付録 D-ストレージのトラブルシューティングについて-データベースのサイズと比べて少なくとも RAM を提供する環境は、実行可能な選択肢ではありません。

これらの推奨事項が存在する理由を理解しておくと、ストレージテクノロジの変更に対応できるようになります。 これらの推奨事項は、次の2つの理由で存在します。 1つ目は IO の分離であり、オペレーティングシステムのスピンドルにおけるパフォーマンスの問題 (つまり、ページング) はデータベースと i/o プロファイルのパフォーマンスに影響しません。 2つ目の方法は、AD DS (およびほとんどのデータベース) のログファイルがシーケンシャルであることです。また、スピンドルベースのハードドライブとキャッシュには、オペレーティングシステムのよりランダムな i/o パターンと、AD DS データベースドライブのほぼ純粋なランダム i/o パターンと比較して、順次 i/o を使用するとパフォーマンス上の大きなメリットがあります。 順次 i/o を別の物理ドライブに分離することにより、スループットを向上させることができます。 現在のストレージオプションで示されている課題は、これらの推奨事項の背後にある基本的な想定が今後は当てはまりません。 ISCSI、SAN、NAS、仮想ディスクのイメージファイルなど、多くの仮想化された記憶域のシナリオでは、基になる記憶域メディアが複数のホストで共有されるため、"IO の分離" と "順次 i/o の最適化" の両方の側面が完全には否定されます。 実際、このようなシナリオでは、共有メディアにアクセスする他のホストがドメインコントローラーへの応答性を低下させる可能性があります。

記憶域のパフォーマンスの計画では、コールドキャッシュの状態、実行中のキャッシュの状態、およびバックアップ/復元という3つのカテゴリを考慮する必要があります。 コールドキャッシュの状態は、ドメインコントローラーが最初に再起動されたときや Active Directory サービスが再起動されたときに、RAM にデータが Active Directory ない場合などに発生します。 ウォームキャッシュの状態は、ドメインコントローラーが安定した状態にあり、データベースがキャッシュされている場所です。 これらは非常に異なるパフォーマンスプロファイルを実行し、データベース全体をキャッシュするのに十分な RAM があるため、キャッシュがコールドの場合にパフォーマンスを向上させることができないので注意が必要です。 次のような2つのシナリオのパフォーマンス設計を検討できます。コールドキャッシュは "スプリント" で、ウォームキャッシュを使用してサーバーを実行するのは "marathon" です。

コールドキャッシュとウォームキャッシュの両方のシナリオでは、ストレージがディスクからメモリにデータを移動する速度が問題になります。 キャッシュの準備は、時間の経過と共に、データを再利用するクエリが増えるにつれてパフォーマンスが向上するシナリオです。キャッシュヒット率が増加し、ディスクへのアクセスが必要になる頻度が減少します。 その結果、ディスクへの移行によるパフォーマンスへの悪影響が軽減されます。 パフォーマンスが低下するのは、キャッシュがウォームされるのを待機している間だけで、システムに依存する最大の許容サイズに拡張された場合のみです。 このメッセージ交換は、ディスクからデータを取得する速度を簡単にすることができます。また、Active Directory に使用できる IOPS の単純な尺度となります。これは、基になるストレージから使用できる IOPS に対する主観的なものです。 計画の観点から見ると、キャッシュの準備とバックアップ/復元のシナリオは例外的に発生するため、通常は時間がかかりませんが、DC の負荷に対して主観的であるため、一般的な推奨事項は存在しません。ただし、これらのアクティビティはピーク時以外にスケジュールされる点が異なります。

AD DS、ほとんどのシナリオでは読み取り IO が主にあり、通常は90% の読み取り/10% 書き込みの比率です。 読み取り i/o は、ユーザーエクスペリエンスのボトルネックになる傾向があり、書き込み IO によって書き込みパフォーマンスが低下することがよくあります。 Ntds.dit への i/o はほぼランダムであるため、キャッシュは読み取り IO に対して最小限のメリットをもたらす傾向があるため、読み取り i/o プロファイル用にストレージを正しく構成することが非常に重要になります。

通常の運用条件では、記憶域の計画目標は、AD DS からの要求がディスクから返されるまでの待機時間を最小限に抑えます。 これは基本的に、未処理 i/o と保留中の i/o の数がディスクへの経路の数以下であることを意味します。 これを測定するには、さまざまな方法があります。 パフォーマンス監視シナリオでは、一般に、LogicalDisk ( *\<NTDS Database Drive\>* ) \ Avg Disk sec/Read は20ミリ秒未満であることを推奨しています。 必要な動作のしきい値は、記憶域の種類に応じて、可能な限りストレージの速度の近くで、2 ~ 6 ミリ秒 (002 ~ .006 second) の範囲で大幅に小さくする必要があります。

例:

![ストレージの待機時間のグラフ](media/capacity-planning-considerations-storage-latency.png)

グラフの分析:

- **左側の緑色の楕円:** 待機時間は10ミリ秒で一貫性が保たれます。 負荷は 800 IOPS から 2400 IOPS に増加します。 これは、基になるストレージで i/o 要求を処理する速度の絶対的な床面です。 これには、ストレージソリューションの詳細が適用されます。
- **右側にある [ブルゴーニュ] の楕円**スループットは、データコレクションの最後までの緑の円の終了から、待機時間が増加し続けている間はフラットなままです。 これは、要求ボリュームが基になるストレージの物理的な制限を超えると、ストレージサブシステムへの送信を待機している要求がキューに置かれる時間が長くなることを示しています。

このナレッジを適用しています:

- **大規模なグループのメンバーシップをクエリするユーザーへの影響–** この場合、ディスクから 1 MB のデータを読み取る必要があると仮定します。 i/o の量とその所要時間は次のように評価できます。
  - Active Directory データベースページのサイズは 8 KB です。
  - 少なくとも128ページをディスクから読み取る必要があります。
  - キャッシュされていないことを前提として、フロア (10 ミリ秒) では、クライアントに返すために、ディスクからデータが読み込まれるまでに最低1.28 秒かかります。 ストレージのスループットが最大容量を超えており、推奨される最大値でもある20ミリ秒で、エンドユーザーに返すために、データをディスクから取得するのに2.5 秒かかります。
- **キャッシュはどのような速度で実行されますか**。クライアントの負荷がこのストレージのスループットを最大化することを前提として、キャッシュは IO あたり 2400 IOPS 8 KB の速度でウォームロードされ &times; ます。 または、1秒あたり約 20 MB/秒で、53秒ごとに約 1 GB のデータベースを RAM に読み込みます。

> [!NOTE]
> システムがバックアップされている場合や、AD DS がガベージコレクションを実行している場合など、コンポーネントが積極的にディスクに対して読み取りまたは書き込みを行うときは、短時間で待機時間が上昇します。 これらの定期的なイベントに対応するために、計算上に追加のヘッドルームを用意する必要があります。 通常の機能に影響を与えずに、これらのシナリオに対応するために十分なスループットを提供することを目標としています。

ご覧のように、ストレージの設計に基づく物理的な制限は、キャッシュの速度がどれほど速くなるかによって決まります。 キャッシュのウォームは、基になるストレージが提供できる最大速度までの受信クライアント要求です。 ピーク時間中にキャッシュを "ウォームアップ" するスクリプトを実行すると、実際のクライアント要求によって負荷が分散されます。 これは、クライアントが最初に必要とするデータの配信に悪影響を及ぼす可能性があります。これは、設計上、ハードディスクリソースが不足していると、DC に接続しているクライアントに関連しないデータが読み込まれるためです。
